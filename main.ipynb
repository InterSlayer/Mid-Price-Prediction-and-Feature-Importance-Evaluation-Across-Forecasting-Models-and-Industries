{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbQ_4hZK_PM2"
      },
      "source": [
        "# **Mid-Price Prediction and Feature Importance Evaluation Across Forecasting Models and Industries**\n",
        "\n",
        "\n",
        "\n",
        "### Author: Nicholas Ffinch  \n",
        "### University of Edinburgh Business School\n",
        "\n",
        "*Disclaimer - Copilot and ChatGPT were used to debug this code*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVOCfizw_PM5"
      },
      "source": [
        "# Access Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYNaW0bKKTL5"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27iyVCrFKH3t"
      },
      "source": [
        "# Conversion to CSV File\n",
        "This paper uses the *Benchmark Dataset for Mid-Price Forecasting of Limit Order Book Data with Machine Learning Methods* by Adam Ntakaris et al. (2018) available below:\n",
        "\n",
        "https://etsin.fairdata.fi/dataset/73eb48d7-4dbc-4a10-a52a-da745b47a649\n",
        "\n",
        "Run the following code to convert the .txt into .csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v21DYZX9KMsr"
      },
      "outputs": [],
      "source": [
        "def conver_data():\n",
        "    data_train_a = pd.read_csv('/content/drive/MyDrive/Train_Dst_NoAuction_ZScore_CF_1.txt', delimiter = r'\\s+', header=None)\n",
        "    data_train_a.to_csv('/content/drive/MyDrive/Dissertation/Zscore/train_no_auc.csv', sep=';', index=False)\n",
        "\n",
        "    data_train = pd.read_csv('/content/drive/MyDrive/Train_Dst_Auction_ZScore_CF_1.txt', delimiter = r'\\s+', header=None)\n",
        "    data_train.to_csv('/content/drive/MyDrive/Dissertation/Zscore/train.csv', sep=';', index=False)\n",
        "\n",
        "    data_test = pd.read_csv('/content/drive/MyDrive/Test_Dst_Auction_ZScore_CF_1.txt', delimiter = r'\\s+', header=None)\n",
        "    data_test.to_csv('/content/drive/MyDrive/Dissertation/Zscore/test.csv', sep=';', index=False)\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVfmKywR_PM6"
      },
      "source": [
        "# Install all the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hcrMUq0KPU_"
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpYw80bEKPVB"
      },
      "source": [
        "# Run all imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vAn_m6yKPVC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import silhouette_score, mean_squared_error, r2_score, accuracy_score, f1_score, cohen_kappa_score, precision_score, roc_curve, auc\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, Activation, Add, Conv1D, Conv2D, Flatten, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from scipy.special import gamma\n",
        "import scipy.signal as signal\n",
        "from scipy.signal import savgol_filter\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from IPython.display import display\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import shap\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, precision_score\n",
        "import shutil\n",
        "import os\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import warnings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViwsVWKd_PM7"
      },
      "source": [
        "# **Part 1: Preprocessing and Data Preparation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0d9DK-AksyD"
      },
      "source": [
        "# Split data into the 5 stocks\n",
        "\n",
        "\n",
        "### **Stock Segmentation Based on Price Jumps**  \n",
        "\n",
        "This script processes **Z score normalized financial market data** that contains information on **five different stocks**. To extract each individual stock, we identify the **start and end indices** using a segmentation approach based on **the highest mid-price jumps**, as recommended by **Adamantios Ntakaris**, one of the dataset authors.\n",
        "\n",
        "\n",
        "\n",
        "### **Methodology:**\n",
        "1. **Data Normalization**  \n",
        "   - The dataset is already **normalized**, ensuring consistency across features and reducing scale-related distortions.\n",
        "\n",
        "\n",
        "2. **Stock Segmentation Using Price Jumps**  \n",
        "   - The mid-price series is analysed to compute **absolute price changes** (using `np.diff()`).\n",
        "   - The **five largest mid-price jumps** are identified to determine the **most significant segmentation points**.\n",
        "   - These jump indices are used as **boundaries** to extract **five distinct stock datasets**.\n",
        "\n",
        "3. **Training and Test Set Splitting**  \n",
        "   - The dataset (`data`) is divided into **five stock segments** based on the identified segmentation points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDwt8-Xf4wl4"
      },
      "outputs": [],
      "source": [
        "def get_stocks_old(data, MID_PRICE_INDEX = 41, OUTLIERS = 20):\n",
        "  # Extract mid-price data from data_df\n",
        "  mid_prices = data.iloc[:, MID_PRICE_INDEX].values  # Using row 41 from data\n",
        "\n",
        "  # Compute absolute differences in mid-price\n",
        "  mid_price_changes = np.diff(mid_prices)\n",
        "\n",
        "  # Find the indices of the 5 largest changes\n",
        "  largest_changes_indices = np.argsort(mid_price_changes)[-5:]\n",
        "\n",
        "  # Sort indices for readability\n",
        "  largest_changes_indices = np.sort(largest_changes_indices)\n",
        "\n",
        "  # Ensure exactly 5 segments by adding the last index\n",
        "  split_indices = np.concatenate(([0], largest_changes_indices, [len(mid_prices)]))\n",
        "\n",
        "  # Keep only the first 6 split indices (to form exactly 5 segments)\n",
        "  split_indices = split_indices[:6]  # Ensures exactly 5 segments\n",
        "\n",
        "  print(\"Final Split Indices:\", split_indices)\n",
        "\n",
        "  # Create a dictionary to store the 5 stock segments\n",
        "  split_data = {}\n",
        "\n",
        "  for i in range(len(split_indices) - 1):\n",
        "      start, end = split_indices[i], split_indices[i + 1]\n",
        "      split_data[f'stock_{i+1}'] = mid_prices[start:end]\n",
        "\n",
        "  # Convert to DataFrame for better visualization\n",
        "  split_df = pd.DataFrame(dict([(key, pd.Series(value)) for key, value in split_data.items()]))\n",
        "\n",
        "  # Split the Training data matrix into 5 segments based on `split_indices`\n",
        "  data_stock_1 = data.iloc[split_indices[0]:split_indices[1], :]\n",
        "  data_stock_2 = data.iloc[split_indices[1]:split_indices[2], :]\n",
        "  data_stock_3 = data.iloc[split_indices[2]:split_indices[3], :]\n",
        "  data_stock_4 = data.iloc[split_indices[3]:split_indices[4], :]\n",
        "  data_stock_5 = data.iloc[split_indices[4]:, :]  # Ensure the last segment extends to the end\n",
        "\n",
        "  return [data_stock_1, data_stock_2, data_stock_3, data_stock_4, data_stock_5, data]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2sLyCLC_PM8"
      },
      "source": [
        "# Feature Engineering for Mid-Price Prediction\n",
        "\n",
        "The code below implements a range of feature engineering functions that will be called upon later\n",
        "\n",
        "## Rolling Window Utility\n",
        "\n",
        "- `rolling_sum(x, window)`: Computes a rolling sum over a specified window.\n",
        "\n",
        "## Price-Based Features\n",
        "\n",
        "- `get_returns_mid_price`: Calculates mid-price returns (log or simple).\n",
        "- `get_realized_variance`: Cumulative realized variance.\n",
        "- `get_realized_variance_rolling`: Rolling version of realized variance.\n",
        "\n",
        "## Limit Order Book (LOB) Features\n",
        "\n",
        "- `get_BOOK_imbalance`: Computes volume imbalance across LOB levels.\n",
        "- `get_FLOW_imbalance`: Measures flow imbalance (volume changes).\n",
        "- `get_market_depth`: Total market depth at specified LOB levels.\n",
        "\n",
        "## Volatility and Noise Measures\n",
        "\n",
        "- `get_bipower_variation` / `get_bipower_variation_rolling`: Variance using absolute return products.\n",
        "- `get_rolling_std`: Rolling standard deviation.\n",
        "- `get_noise_variance_MP` / `get_noise_variance_MP_rolling`: Microstructure noise variance.\n",
        "- `get_noise_variance_returns` / `_rolling`: Return-based noise estimator.\n",
        "\n",
        "## Advanced Variance Measures\n",
        "\n",
        "- `get_RQ` / `get_RQ_rolling`: Realized quarticity.\n",
        "- `get_RQT` / `get_RQT_rolling`: Tripower quarticity estimator.\n",
        "- `get_realised_kernel`: Robust volatility estimator using autocovariation and Parzen kernel.\n",
        "\n",
        "## Smoothing\n",
        "\n",
        "- `get_ema`: Exponential moving average of price series.\n",
        "\n",
        "> Note: Most rolling versions use a default window size of 30 unless otherwise specified.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EThGHvvgwF5n"
      },
      "outputs": [],
      "source": [
        "WINDOW = 30\n",
        "\n",
        "\n",
        "# Helper function for rolling sum with a given window size.\n",
        "def rolling_sum(x, window):\n",
        "    result = np.empty(len(x))\n",
        "    for i in range(len(x)):\n",
        "        start = max(0, i - window + 1)\n",
        "        result[i] = np.sum(x[start:i+1])\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################################\n",
        "#                            Feature Engineering Functions                          #\n",
        "#####################################################################################\n",
        "\n",
        "# Define the features\n",
        "def get_returns_mid_price(mid_price, use_log_returns = False):\n",
        "    if use_log_returns:\n",
        "        return np.diff(np.log(mid_price))\n",
        "\n",
        "    return np.diff(mid_price)\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def get_realized_variance(returns, use_log_returns=False):\n",
        "\n",
        "    # Calculate the realised variance as the sum of squared returns\n",
        "    rv = np.empty(returns.size+1)\n",
        "    rv[0] = np.nan\n",
        "    rv[1:] = np.cumsum(returns ** 2)\n",
        "    return rv\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# Modified realised variance: rolling sum of squared returns over a window of 10 events.\n",
        "def get_realized_variance_rolling(returns, window=10, use_log_returns=False):\n",
        "    rv = np.empty(returns.size + 1)\n",
        "    rv[0] = np.nan\n",
        "    # Instead of a cumulative sum over all returns, compute the rolling sum.\n",
        "    rv[1:] = rolling_sum(returns ** 2, window)\n",
        "    return rv\n",
        "\n",
        "################################################################################\n",
        "\n",
        "#TODO\n",
        "def get_BOOK_imbalance(data_matrix, level = 5):\n",
        "    # Establish indexes in the data\n",
        "    ask_volumes = np.zeros(data_matrix.shape[1])\n",
        "    bid_volumes = np.zeros(data_matrix.shape[1])\n",
        "    for i in range(level):\n",
        "        index_1 = 1 + 4 * i\n",
        "        index_2 = 3 + 4 * i\n",
        "        ask_volumes += data_matrix[index_1, :]\n",
        "        bid_volumes += data_matrix[index_2, :]\n",
        "\n",
        "    denominator = ask_volumes + bid_volumes\n",
        "    weighted_imbalance = np.where(denominator != 0,(ask_volumes - bid_volumes) / (ask_volumes + bid_volumes),0)\n",
        "    return weighted_imbalance\n",
        "\n",
        "################################################################################\n",
        "\n",
        "#TODO\n",
        "def get_FLOW_imbalance(data_matrix, level = 1):\n",
        "    # Establish indexes in the data\n",
        "    index_1 = 1 + 4 * (level - 1)\n",
        "    index_2 = 3 + 4 * (level - 1)\n",
        "    ask_volumes = data_matrix[index_1, :]\n",
        "    bid_volumes = data_matrix[index_2, :]\n",
        "\n",
        "    # Get the previous bid and ask volumes\n",
        "    bid_volumes_prev = np.roll(bid_volumes, 1)\n",
        "    ask_volumes_prev = np.roll(ask_volumes, 1)\n",
        "\n",
        "    # Calculate the imbalance\n",
        "    imbalance = ((bid_volumes - bid_volumes_prev) - (ask_volumes - ask_volumes_prev))\n",
        "    return imbalance\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def get_market_depth(data_matrix, level = 5):\n",
        "    depth = np.zeros(data_matrix.shape[1])\n",
        "    for i in range(level):\n",
        "        index_1 = 1 + 4 * i\n",
        "        index_2 = 3 + 4 * i\n",
        "        depth += data_matrix[index_1, :] + data_matrix[index_2, :]\n",
        "    return depth\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# Realised Kernel Calculation\n",
        "def get_parzen_kernel(x):\n",
        "    \"\"\"Parzen kernel function for noise-robust QV estimation.\"\"\"\n",
        "    abs_x = np.abs(x)\n",
        "    if abs_x <= 0.5:\n",
        "        return 1 - 6 * (abs_x ** 2) + 6 * (abs_x ** 3)\n",
        "    elif abs_x <= 1:\n",
        "        return 2 * ((1 - abs_x) ** 3)\n",
        "    return 0  # Zero outside the range\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def get_autocovariation(X, h):\n",
        "    \"\"\"Compute autocovariation at lag h.\"\"\"\n",
        "    if h == 0:\n",
        "        return np.var(X, ddof=1)  # Variance at lag 0\n",
        "    return np.mean((X[:-h] - np.mean(X)) * (X[h:] - np.mean(X)))  # Covariance at lag h\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def get_realised_kernel(X, H=10):\n",
        "    N = len(X)\n",
        "    RK = np.zeros(N)\n",
        "\n",
        "    for t in range(N):\n",
        "        # Compute the lag-0 autocovariation using data up to time t.\n",
        "        gamma_0 = get_autocovariation(X[:t+1], 0)\n",
        "        kernel_sum = 0.0\n",
        "\n",
        "        # For each lag h, ensure we have enough observations (i.e., t >= h)\n",
        "        for h in range(1, min(H, t) + 1):\n",
        "            # Weight for the h-th lag from the Parzen kernel function.\n",
        "            weight = get_parzen_kernel(h / H)\n",
        "            # Compute the autocovariations at lag h and -h using data up to t.\n",
        "            gamma_h = get_autocovariation(X[:t+1], h)\n",
        "            gamma_minus_h = get_autocovariation(X[:t+1], -h)\n",
        "            kernel_sum += weight * (gamma_h + gamma_minus_h)\n",
        "\n",
        "        # The cumulative realized kernel at time t.\n",
        "        RK[t] = gamma_0 + kernel_sum\n",
        "\n",
        "    return RK\n",
        "\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def get_preaveraged_realized_variance(prices, returns, delta_n, theta=1):\n",
        "\n",
        "    n = len(prices)\n",
        "    # Determine pre-averaging window size H (must be at least 2)\n",
        "    H = max(int(np.floor(theta * np.sqrt(n))), 2)\n",
        "\n",
        "    # Define the pre-averaging function g(x) = min(x, 1-x) for x in [0,1]\n",
        "    def g(x):\n",
        "        return min(x, 1 - x)\n",
        "\n",
        "    # Compute the sum of squared pre-averaged increments\n",
        "    preavg_squared_sum = 0.0\n",
        "    # The index i runs from 0 to n - H + 1 (inclusive)\n",
        "    for i in range(n - H + 2):\n",
        "        preavg = 0.0\n",
        "        for j in range(1, H):\n",
        "            preavg += g(j / H) * returns[i + j - 1]\n",
        "        preavg_squared_sum += preavg ** 2\n",
        "\n",
        "    # Set psi values for g(x) = min(x, 1-x)\n",
        "    psi1 = 1.0\n",
        "    psi2 = 1.0 / 12.0\n",
        "\n",
        "    # Sum of squared returns (the realized variance component)\n",
        "    sum_squared_returns = np.sum(returns ** 2)\n",
        "\n",
        "    # Compute the pre-averaged realized variance (PA-RV)\n",
        "    PA_RV = (np.sqrt(delta_n) / (theta * psi2)) * preavg_squared_sum - \\\n",
        "            (psi1 * delta_n / (2 * theta**2 * psi2)) * sum_squared_returns\n",
        "\n",
        "    return PA_RV\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def get_bipower_variation(r):\n",
        "\n",
        "    # Compute the product of consecutive absolute returns:\n",
        "    # For i=1,...,len(r)-1, prod[i-1] = |r[i]| * |r[i-1]|\n",
        "\n",
        "    prod = np.abs(r[1:]) * np.abs(r[:-1])\n",
        "    cum_prod = np.cumsum(prod)\n",
        "\n",
        "    # Allocate an array for BV with the same length as prices\n",
        "    BV = np.full(r.shape, np.nan)\n",
        "    # For t >= 2 (i.e. price index 2 corresponds to the first full pair), assign:\n",
        "    return np.array((np.pi / 2) * cum_prod)\n",
        "\n",
        "# Modified bipower variation: rolling sum of products of consecutive absolute returns.\n",
        "def get_bipower_variation_rolling(r, window=WINDOW):\n",
        "    # Calculate element-wise product for consecutive absolute returns.\n",
        "    prod = np.abs(r[1:]) * np.abs(r[:-1])\n",
        "    # Compute the rolling sum over the window.\n",
        "    rolling_prod = rolling_sum(prod, window)\n",
        "    # Multiply by the constant.\n",
        "    # Note: The length of rolling_prod is the same as prod; you may want to pad to match r.\n",
        "    BV = np.full(len(r), np.nan)\n",
        "    # We assign the rolling bipower variation starting from index 1 (or adjust as needed)\n",
        "    BV[1:len(rolling_prod)+1] = (np.pi / 2) * rolling_prod\n",
        "    return BV\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def get_rolling_std(returns, window = WINDOW):\n",
        "\n",
        "    n = len(returns)\n",
        "    rolling_std = np.full(n, np.nan)\n",
        "\n",
        "    # For each time t where we have a full window, compute the rolling standard deviation.\n",
        "    for t in range(0, n):\n",
        "        if n-t-window == 0:\n",
        "            window -= 1\n",
        "        window_returns = returns[t:t + window]\n",
        "        mean_return = np.mean(window_returns)\n",
        "        # Note: Using 1/N as in the provided formula (i.e. the population standard deviation).\n",
        "        rolling_std[t] = np.sqrt(np.mean((window_returns - mean_return) ** 2))\n",
        "\n",
        "    return rolling_std\n",
        "\n",
        "################################################################################\n",
        "def get_noise_variance_MP (returns):\n",
        "    n = len(returns)\n",
        "    prod = returns[1:] * returns[:-1]\n",
        "    return(-1/(n-1) * np.cumsum(prod))\n",
        "\n",
        "# Modified noise variance (MP): rolling sum version.\n",
        "def get_noise_variance_MP_rolling(returns, window=WINDOW):\n",
        "    # Compute product of consecutive returns.\n",
        "    prod = returns[1:] * returns[:-1]\n",
        "    # Rolling sum over the product.\n",
        "    rolling_prod = rolling_sum(prod, window)\n",
        "    # Instead of dividing by (n-1), you might normalize by (window-1) to reflect the window length.\n",
        "    return -1 / (window - 1) * rolling_prod\n",
        "################################################################################\n",
        "\n",
        "def get_noise_variance_returns (returns):\n",
        "    n = len(returns)\n",
        "    return(1/(2*n) * np.cumsum(returns**2))\n",
        "\n",
        "# Modified noise variance based on returns: rolling sum version.\n",
        "def get_noise_variance_returns_rolling(returns, window=WINDOW):\n",
        "    rolling_sq = rolling_sum(returns ** 2, window)\n",
        "    return 1 / (2 * window) * rolling_sq\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def get_RQ(returns):\n",
        "    return(len(returns)/3 *np.cumsum(returns**4))\n",
        "\n",
        "# Modified RQ: rolling sum of fourth powers.\n",
        "def get_RQ_rolling(returns, window=WINDOW):\n",
        "    # Rolling sum of r^4 over the window.\n",
        "    rolling_r4 = rolling_sum(returns ** 4, window)\n",
        "    # Normalize similarly to your original (originally len(returns)/3, now window/3)\n",
        "    return (window / 3) * rolling_r4\n",
        "\n",
        "################################################################################\n",
        "def get_RQT(returns):\n",
        "\n",
        "    n = len(returns)\n",
        "    if n < 3:\n",
        "        raise ValueError(\"Return series must contain at least 3 observations.\")\n",
        "\n",
        "    # Compute μ₄/₃ = 2^(2/3) * Γ(7/6) / sqrt(pi)\n",
        "    mu_4_3 = 2**(2/3) * gamma(7/6) / np.sqrt(np.pi)\n",
        "\n",
        "    # Compute |r|^(4/3) for each return\n",
        "    r_power = np.abs(returns) ** (4/3)\n",
        "\n",
        "    # For i = 3, ..., n (1-indexed), the product term is:\n",
        "    # |r_i|^(4/3) * |r_{i-1}|^(4/3) * |r_{i-2}|^(4/3)\n",
        "    # In Python (0-indexed), sum over indices i = 2 to n-1:\n",
        "    product_terms = r_power[2:] * r_power[1:-1] * r_power[:-2]\n",
        "\n",
        "    sum_product = np.cumsum(product_terms)\n",
        "\n",
        "    RQ_t = n * (mu_4_3 ** -3) * sum_product\n",
        "    return RQ_t\n",
        "\n",
        "# Modified RQT: rolling sum of products of three consecutive |r|^(4/3)\n",
        "def get_RQT_rolling(returns, window=WINDOW):\n",
        "    n = len(returns)\n",
        "    if n < 3:\n",
        "        raise ValueError(\"Return series must contain at least 3 observations.\")\n",
        "    # Constant from your original code.\n",
        "    mu_4_3 = 2**(2/3) * gamma(7/6) / np.sqrt(np.pi)\n",
        "    r_power = np.abs(returns) ** (4/3)\n",
        "    # Compute product terms for triple consecutive observations.\n",
        "    product_terms = r_power[:-2] * r_power[1:-1] * r_power[2:]\n",
        "    # Compute the rolling sum over these product terms.\n",
        "    rolling_prod = rolling_sum(product_terms, window)\n",
        "    return window * (mu_4_3 ** -3) * rolling_prod\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def get_ema(prices, period = 10):\n",
        "\n",
        "    n = len(prices)\n",
        "    if n == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    alpha = 2 / (period + 1)\n",
        "    ema = np.empty(n)\n",
        "    ema[:period] = np.mean(prices[:period]) #Mean of the first period\n",
        "\n",
        "    for t in range(1, n):\n",
        "        ema[t] = alpha * prices[t] + (1 - alpha) * ema[t - 1]\n",
        "\n",
        "    return ema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzT0kpJmzmes"
      },
      "source": [
        "## **Feature Extraction for Market Data Analysis**\n",
        "This script defines the function **`get_features(fdata, level=5)`**, which extracts a wide range of features from market data for high-frequency trading analysis. It processes **limit order book (LOB) data** and computes various financial metrics.\n",
        "\n",
        "#### **Key Operations:**\n",
        "1. **Data Transformation**  \n",
        "   - Converts the input dataframe into a NumPy array for faster computations.\n",
        "\n",
        "2. **Mid-Price Calculation**  \n",
        "   - Extracts mid-prices at five depth levels.\n",
        "\n",
        "3. **Bid-Ask Spread Calculation**  \n",
        "   - Computes bid-ask spreads across five depth levels.\n",
        "\n",
        "4. **Return Calculations**  \n",
        "   - Derives log returns for mid-prices.\n",
        "\n",
        "5. **Price Differences Across Levels**  \n",
        "   - Computes price differences between adjacent bid and ask levels.\n",
        "\n",
        "6. **Statistical Features**  \n",
        "   - Calculates mean prices and volumes for bid/ask sides.  \n",
        "   - Computes accumulated price and volume differences.\n",
        "\n",
        "7. **Time-Sensitive Features**  \n",
        "   - Computes price and volume derivatives over time.  \n",
        "   - Extracts trade intensity and limit activity acceleration.\n",
        "\n",
        "8. **Order Flow & Market Dynamics**  \n",
        "   - Computes **Order Flow Imbalance (OFI)** and **Order Book Imbalance (OBI)**.  \n",
        "   - Measures **Market Depth**.  \n",
        "\n",
        "9. **Advanced Financial Metrics**  \n",
        "   - Computes **Realized Variance, Kernel Variance, Bipower Variance, Jump Variation, and Rolling Standard Deviation**.  \n",
        "   - Estimates **Noise Variance** for mid-price and returns.  \n",
        "   - Computes **Exponential Moving Averages (EMA)** and **Realized Quadratic Variations (RQ & RQT)**.\n",
        "\n",
        "10. **Feature Dictionary Output**  \n",
        "   - Returns a structured dictionary containing all extracted features for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1XBqKvqkd-b"
      },
      "outputs": [],
      "source": [
        "\n",
        "#####################################################################################\n",
        "#                                Feature Building                                   #\n",
        "#####################################################################################\n",
        "\n",
        "def get_features(fdata, level = 5):\n",
        "    # create a matrix of the data\n",
        "    data_matrix = fdata.to_numpy()\n",
        "\n",
        "\n",
        "    # Calculate the mid price\n",
        "    mid_price_l1 = data_matrix[41, :]\n",
        "    mid_price_l2 = data_matrix[43, :]\n",
        "    mid_price_l3 = data_matrix[45, :]\n",
        "    mid_price_l4 = data_matrix[47, :]\n",
        "    mid_price_l5 = data_matrix[49, :]\n",
        "\n",
        "    # Calculate bid ask spread\n",
        "    spread_l1 = data_matrix[40, :]\n",
        "    spread_l2 = data_matrix[42, :]\n",
        "    spread_l3 = data_matrix[44, :]\n",
        "    spread_l4 = data_matrix[46, :]\n",
        "    spread_l5 = data_matrix[48, :]\n",
        "\n",
        "\n",
        "    # Get returns for the midprice assumed log return\n",
        "    mid_price_returns_l1 = get_returns_mid_price(mid_price_l1)\n",
        "    mid_price_returns_l2 = get_returns_mid_price(mid_price_l2)\n",
        "    mid_price_returns_l3 = get_returns_mid_price(mid_price_l3)\n",
        "    mid_price_returns_l4 = get_returns_mid_price(mid_price_l4)\n",
        "    mid_price_returns_l5 = get_returns_mid_price(mid_price_l5)\n",
        "\n",
        "    # Get Price differences from data set\n",
        "\n",
        "    Price_diff_ask_l1_l2 = data_matrix[62,:]\n",
        "    Price_diff_ask_l2_l3 = data_matrix[64,:]\n",
        "    Price_diff_ask_l3_l4 = data_matrix[66,:]\n",
        "    Price_diff_ask_l4_l5 = data_matrix[68,:]\n",
        "    Price_diff_ask_l1_l5 = data_matrix[16,:] - data_matrix[0,:]\n",
        "\n",
        "    Price_diff_bid_l1_l2 = data_matrix[63,:]\n",
        "    Price_diff_bid_l2_l3 = data_matrix[65,:]\n",
        "    Price_diff_bid_l3_l4 = data_matrix[67,:]\n",
        "    Price_diff_bid_l4_l5 = data_matrix[69,:]\n",
        "    Price_diff_bid_l1_l5 = data_matrix[2,:] - data_matrix[19,:]\n",
        "\n",
        "\n",
        "    # Get Price and Volume Means\n",
        "    Price_mean_ask = data_matrix[80,:]\n",
        "    Price_mean_bid = data_matrix[81,:]\n",
        "    Vol_mean_ask = data_matrix[82,:]\n",
        "    Vol_mean_bid = data_matrix[83,:]\n",
        "\n",
        "    # Accumulated differences\n",
        "    accumulated_diff_price = data_matrix[84,:]\n",
        "    accumulated_diff_volume = data_matrix[85,:]\n",
        "\n",
        "    ############################################################################\n",
        "    #                      EXTRACT Time Insensitive DATA                       #\n",
        "    ############################################################################\n",
        "\n",
        "    # Extract derivation over dt\n",
        "\n",
        "    # Derivation on price dP\n",
        "    dP_ask_l1 = data_matrix[86,:]\n",
        "    dP_ask_l2 = data_matrix[90,:]\n",
        "    dP_ask_l3 = data_matrix[94,:]\n",
        "    dP_ask_l4 = data_matrix[98,:]\n",
        "    dP_ask_l5 = data_matrix[102,:]\n",
        "\n",
        "    dP_bid_l1 = data_matrix[87,:]\n",
        "    dP_bid_l2 = data_matrix[91,:]\n",
        "    dP_bid_l3 = data_matrix[95,:]\n",
        "    dP_bid_l4 = data_matrix[99,:]\n",
        "    dP_bid_l5 = data_matrix[103,:]\n",
        "\n",
        "    # Derivation on volume dV\n",
        "    dV_ask_l1 = data_matrix[88,:]\n",
        "    dV_ask_l2 = data_matrix[92,:]\n",
        "    dV_ask_l3 = data_matrix[96,:]\n",
        "    dV_ask_l4 = data_matrix[100,:]\n",
        "    dV_ask_l5 = data_matrix[104,:]\n",
        "\n",
        "    dV_bid_l1 = data_matrix[89,:]\n",
        "    dV_bid_l2 = data_matrix[93,:]\n",
        "    dV_bid_l3 = data_matrix[97,:]\n",
        "    dV_bid_l4 = data_matrix[101,:]\n",
        "    dV_bid_l5 = data_matrix[105,:]\n",
        "\n",
        "\n",
        "    # Extract Average Intensity per type - 6 types\n",
        "    intensity_1 = data_matrix[126,:]\n",
        "    intensity_2 = data_matrix[127,:]\n",
        "    intensity_3 = data_matrix[128,:]\n",
        "    intensity_4 = data_matrix[129,:]\n",
        "    intensity_5 = data_matrix[130,:]\n",
        "    intensity_6 = data_matrix[131,:]\n",
        "\n",
        "    # Extract Relative Intensity Comparison - 6 types\n",
        "    intensity_comp_1 = data_matrix[132,:]\n",
        "    intensity_comp_2 = data_matrix[133,:]\n",
        "    intensity_comp_3 = data_matrix[134,:]\n",
        "    intensity_comp_4 = data_matrix[135,:]\n",
        "    intensity_comp_5 = data_matrix[136,:]\n",
        "    intensity_comp_6 = data_matrix[137,:]\n",
        "\n",
        "    # Extract Limit Activity Acceleration - 6 types\n",
        "    LAA_1 = data_matrix[138,:]\n",
        "    LAA_2 = data_matrix[139,:]\n",
        "    LAA_3 = data_matrix[140,:]\n",
        "    LAA_4 = data_matrix[141,:]\n",
        "    LAA_5 = data_matrix[142,:]\n",
        "    LAA_6 = data_matrix[143,:]\n",
        "\n",
        "    # Get Price Movement\n",
        "    # -1 because we count from 0\n",
        "    movement_1 = data_matrix[144,:] -1 # t+1\n",
        "    movement_5 = data_matrix[147,:] -1 # t+5\n",
        "    movement_6 = data_matrix[148,:] -1 # t+10\n",
        "\n",
        "    #Realised Variance\n",
        "    realised_variance = get_realized_variance_rolling(mid_price_returns_l1)\n",
        "\n",
        "    # Calculate the the order flow imbalance\n",
        "\n",
        "    OFI = get_FLOW_imbalance(data_matrix)[1:]\n",
        "\n",
        "    # Calculate the the order flow imbalance\n",
        "    OBI = get_BOOK_imbalance(data_matrix)[1:]\n",
        "\n",
        "    # # Calculate Market Depth\n",
        "    Market_depth = get_market_depth(data_matrix)[1:]\n",
        "\n",
        "    # # Calculate the financial duration\n",
        "    # #times = message_matrix[:, 0]\n",
        "    # #financial_duration = np.diff(times)\n",
        "\n",
        "    # # Average mid price financial duration\n",
        "    # #AMPD = financial_duration * mid_prices[1:]\n",
        "\n",
        "    # # Cancelation rate calculation\n",
        "    # #cancelation_rate = get_cancellation_ratio(message_matrix)[1:]\n",
        "\n",
        "    # Calculate the realized kernel\n",
        "    realised_kernel_mid_prices = get_realised_kernel(mid_price_l1)[1:]\n",
        "\n",
        "    # # Calculate preaveraged realized variance\n",
        "    # PRV = get_preaveraged_realized_variance(mid_price_l1, mid_price_returns_l1, 1)\n",
        "    # # Calculate the exponential moving average\n",
        "    ema_mid_prices = get_ema(mid_price_l1, 10)\n",
        "\n",
        "    # # Calculate Bipower Variance\n",
        "    BV = get_bipower_variation_rolling(mid_price_returns_l1)\n",
        "    # print(BV.shape)\n",
        "\n",
        "    # # Calculate Jump Variation\n",
        "    JV = realised_variance[1:] - BV\n",
        "\n",
        "    # print(realised_variance.shape)\n",
        "    # # Calculate Rolling Standard D  eviation\n",
        "    R_std_dev = get_rolling_std(mid_price_returns_l1, 0)\n",
        "\n",
        "    NV_MP = get_noise_variance_MP_rolling(mid_price_returns_l1)\n",
        "    NV_returns = get_noise_variance_returns_rolling(mid_price_returns_l1)\n",
        "    RQ = get_RQ_rolling(mid_price_returns_l1)\n",
        "    RQT = get_RQT_rolling(mid_price_returns_l1)\n",
        "\n",
        "\n",
        "    return {\n",
        "    # Features extracted from data\n",
        "\n",
        "    ########################################################################\n",
        "    #                          U2 Time-Insensitive                         #\n",
        "    ########################################################################\n",
        "\n",
        "    # Mid prices\n",
        "    'mid_prices': np.array(mid_price_l1[3:]),\n",
        "    'mid_prices_level_2': np.array(mid_price_l2[3:]),\n",
        "    'mid_prices_level_3': np.array(mid_price_l3[3:]),\n",
        "    'mid_prices_level_4': np.array(mid_price_l4[3:]),\n",
        "    'mid_prices_level_5': np.array(mid_price_l5[3:]),\n",
        "\n",
        "    ##Spreads\n",
        "    'spread_level_1': np.array(spread_l1[3:]),\n",
        "    'spread_level_2': np.array(spread_l2[3:]),\n",
        "    'spread_level_3': np.array(spread_l3[3:]),\n",
        "    'spread_level_4': np.array(spread_l4[3:]),\n",
        "    'spread_level_5': np.array(spread_l5[3:]),\n",
        "\n",
        "    ## Mid price returns\n",
        "    'mid_price_returns_level_1': np.array(mid_price_returns_l1[2:]),\n",
        "    'mid_price_returns_level_2': np.array(mid_price_returns_l2[2:]),\n",
        "    'mid_price_returns_level_3': np.array(mid_price_returns_l3[2:]),\n",
        "    'mid_price_returns_level_4': np.array(mid_price_returns_l4[2:]),\n",
        "    'mid_price_returns_level_5': np.array(mid_price_returns_l5[2:]),\n",
        "\n",
        "    ## Price difference between levels\n",
        "    # ASK\n",
        "    'Price_diff_ask_l1_l2': np.array(Price_diff_ask_l1_l2[3:]),\n",
        "    'Price_diff_ask_l2_l3': np.array(Price_diff_ask_l2_l3[3:]),\n",
        "    'Price_diff_ask_l3_l4': np.array(Price_diff_ask_l3_l4[3:]),\n",
        "    'Price_diff_ask_l4_l5': np.array(Price_diff_ask_l4_l5[3:]),\n",
        "    'Price_diff_ask_l1_l5': np.array(Price_diff_ask_l1_l5[3:]),\n",
        "\n",
        "    ## BID\n",
        "    'Price_diff_bid_l1_l2': np.array(Price_diff_bid_l1_l2[3:]),\n",
        "    'Price_diff_bid_l2_l3': np.array(Price_diff_bid_l2_l3[3:]),\n",
        "    'Price_diff_bid_l3_l4': np.array(Price_diff_bid_l3_l4[3:]),\n",
        "    'Price_diff_bid_l4_l5': np.array(Price_diff_bid_l4_l5[3:]),\n",
        "    'Price_diff_bid_l1_l5': np.array(Price_diff_bid_l1_l5[3:]),\n",
        "\n",
        "    # Get Price and Volume Means\n",
        "    'Price_mean_ask': np.array(Price_mean_ask[3:]),\n",
        "    'Price_mean_bid': np.array(Price_mean_bid[3:]),\n",
        "    'Vol_mean_ask': np.array(Vol_mean_ask[3:]),\n",
        "    'Vol_mean_bid': np.array(Vol_mean_bid[3:]),\n",
        "\n",
        "    ## Accumulated differences\n",
        "    'accumulated_diff_price': np.array(accumulated_diff_price[3:]),\n",
        "    'accumulated_diff_volume': np.array(accumulated_diff_volume[3:]),\n",
        "\n",
        "    ########################################################################\n",
        "    #                           U3 Time-Sensitive                          #\n",
        "    ########################################################################\n",
        "\n",
        "    ## Derivation on price dP and volume dV\n",
        "    #ASK - Price\n",
        "    'dP_ask_l1': np.array(dP_ask_l1[3:]),\n",
        "    'dP_ask_l2': np.array(dP_ask_l2[3:]),\n",
        "    'dP_ask_l3': np.array(dP_ask_l3[3:]),\n",
        "    'dP_ask_l4': np.array(dP_ask_l4[3:]),\n",
        "    'dP_ask_l5': np.array(dP_ask_l5[3:]),\n",
        "\n",
        "    #BID - Price\n",
        "    'dP_bid_l1': np.array(dP_bid_l1[3:]),\n",
        "    'dP_bid_l2': np.array(dP_bid_l2[3:]),\n",
        "    'dP_bid_l3': np.array(dP_bid_l3[3:]),\n",
        "    'dP_bid_l4': np.array(dP_bid_l4[3:]),\n",
        "    'dP_bid_l5': np.array(dP_bid_l5[3:]),\n",
        "\n",
        "    #ASK - Volume\n",
        "    'dV_ask_l1': np.array(dV_ask_l1[3:]),\n",
        "    'dV_ask_l2': np.array(dV_ask_l2[3:]),\n",
        "    'dV_ask_l3': np.array(dV_ask_l3[3:]),\n",
        "    'dV_ask_l4': np.array(dV_ask_l4[3:]),\n",
        "    'dV_ask_l5': np.array(dV_ask_l5[3:]),\n",
        "\n",
        "    #BID - Volume\n",
        "    'dV_bid_l1': np.array(dV_bid_l1[3:]),\n",
        "    'dV_bid_l2': np.array(dV_bid_l2[3:]),\n",
        "    'dV_bid_l3': np.array(dV_bid_l3[3:]),\n",
        "    'dV_bid_l4': np.array(dV_bid_l4[3:]),\n",
        "    'dV_bid_l5': np.array(dV_bid_l5[3:]),\n",
        "\n",
        "    ## Extract Average Intensity per type - 6 types\n",
        "    'intensity_1': np.array(intensity_1[3:]),\n",
        "    'intensity_2': np.array(intensity_2[3:]),\n",
        "    'intensity_3': np.array(intensity_3[3:]),\n",
        "    'intensity_4': np.array(intensity_4[3:]),\n",
        "    'intensity_5': np.array(intensity_5[3:]),\n",
        "    'intensity_6': np.array(intensity_6[3:]),\n",
        "\n",
        "    ## Extract Relative Intensity Comparison - 6 types\n",
        "    'intensity_comp_1': np.array(intensity_comp_1[3:]),\n",
        "    'intensity_comp_2': np.array(intensity_comp_2[3:]),\n",
        "    'intensity_comp_3': np.array(intensity_comp_3[3:]),\n",
        "    'intensity_comp_4': np.array(intensity_comp_4[3:]),\n",
        "    'intensity_comp_5': np.array(intensity_comp_5[3:]),\n",
        "    'intensity_comp_6': np.array(intensity_comp_6[3:]),\n",
        "\n",
        "    # Extract Limit Activity Acceleration - 6 types\n",
        "    'LAA_1': np.array(LAA_1[3:]),\n",
        "    'LAA_2': np.array(LAA_2[3:]),\n",
        "    'LAA_3': np.array(LAA_3[3:]),\n",
        "    'LAA_4': np.array(LAA_4[3:]),\n",
        "    'LAA_5': np.array(LAA_5[3:]),\n",
        "    'LAA_6': np.array(LAA_6[3:]),\n",
        "\n",
        "    ########################################################################\n",
        "    #                           Computed features                          #\n",
        "    ########################################################################\n",
        "\n",
        "    'realised_variance': np.array(realised_variance[3:]),\n",
        "    'Order Flow Imbalance': np.array(OFI[2:]),\n",
        "    'Order Book Imbalance': np.array(OBI[2:]),\n",
        "    'Market Depth': np.array(Market_depth[2:]),\n",
        "    'Realised Kernel': np.array(realised_kernel_mid_prices[2:]),\n",
        "    # 'Preaverage Realised variance': np.array(PRV)\n",
        "    'Exponential Moving Average': np.array(ema_mid_prices[3:]),\n",
        "    'Bipower Variance': np.array(BV[2:]),\n",
        "    'Jump Variation': np.array(JV[2:]),\n",
        "    # 'Rolling Standard Deviation': np.array(R_std_dev[2:]),\n",
        "    'Noise Variance MP': np.array(NV_MP[1:]),\n",
        "    'Noise Variance Returns': np.array(NV_returns[2:]),\n",
        "    'RQ': np.array(RQ[2:]),\n",
        "    'RQT': np.array(RQT),\n",
        "    'Price Movement t+1': np.array(movement_1[3:]),\n",
        "    'Price Movement t+5': np.array(movement_5[3:]),\n",
        "    'Price Movement t+10': np.array(movement_6[3:])\n",
        "    }\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owaUuonVZLKU"
      },
      "source": [
        "# Data Visualization and Diagnostics\n",
        "\n",
        "This script includes functions for visualizing mid-price series, analyzing correlation between features, and computing multicollinearity using VIF.\n",
        "\n",
        "### `plot_stocks`\n",
        "Generates and saves smoothed mid-price plots for multiple stocks using moving averages. Each stock is plotted individually and saved as a PNG image.\n",
        "\n",
        "### `plot_corr_matrix`\n",
        "Creates and saves correlation matrix heatmaps for each stock dataset. Depending on the Correlated flag, output is stored in different subfolders.\n",
        "\n",
        "### `save_vif_analysis`\n",
        "Computes Variance Inflation Factor (VIF) for each stock's features to assess multicollinearity. Results are saved as CSV files.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOWbKH_XZW31"
      },
      "outputs": [],
      "source": [
        "def plot_stocks(dfs, path, smoothing_window= 300, OUTLIERS = 50):\n",
        "\n",
        "    # Define file path\n",
        "    file_path = os.path.join(path, 'Mid_Price_Graph')\n",
        "\n",
        "    # Define a moving average smoothing function.\n",
        "    def smooth_series(series, window=smoothing_window):\n",
        "        return np.convolve(series, np.ones(window) / window, mode='valid')\n",
        "\n",
        "    num_stocks = len(dfs)  # Adjust if needed\n",
        "\n",
        "    # Ensure the target directory exists.\n",
        "    os.makedirs(file_path, exist_ok=True)\n",
        "\n",
        "    # Loop over each stock.\n",
        "    for i, df in enumerate(dfs):\n",
        "        # Assume OUTLIERS is a global constant defined elsewhere\n",
        "        stock_data = df['mid_prices'][OUTLIERS:-OUTLIERS]\n",
        "        smoothed_stock = smooth_series(stock_data)\n",
        "\n",
        "        i += 1\n",
        "        # Create a new figure for each stock.\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(smooth_series(stock_data, 20), label=f'Mid Price Stock {i}', color='blue')\n",
        "        plt.plot(smoothed_stock, label=f'Moving Average Stock {i}', color='red')\n",
        "        plt.xlabel('Events')\n",
        "        plt.ylabel('Mid-Price')\n",
        "        plt.title(f'Plot for the Stock #{i} mid-price')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Save the figure with a unique name.\n",
        "        utput_file = os.path.join(file_path, f\"Plot_for_the_{i}_mid_price.png\")\n",
        "        plt.savefig(output_file)\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Plot saved to: {output_file}\")\n",
        "\n",
        "    # Example usage (assuming split_data dictionary and OUTLIERS constant are defined):\n",
        "    # plot_and_save_each_stock_mid_price(split_data)\n",
        "    return\n",
        "\n",
        "def plot_corr_matrix(dfs, path, Correlated = False):\n",
        "\n",
        "    if Correlated == True:\n",
        "        file_path = os.path.join(path, 'Correlation_Matrix_Clean')\n",
        "    else:\n",
        "        file_path = os.path.join(path, 'Correlation_Matrix_No_Corr')\n",
        "    # Loop over each DataFrame in the list and save its correlation matrix plot\n",
        "\n",
        "    for i, df in enumerate(dfs, start=1):\n",
        "        # Compute the correlation matrix\n",
        "        corr_matrix = df.corr()\n",
        "\n",
        "        # Create a figure for the heatmap\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "        plt.title(f'Correlation Matrix of Dataset {i}')\n",
        "\n",
        "        # Create the full file path\n",
        "        output_file = os.path.join(file_path, f\"correlation_matrix_stock_{i}.png\")\n",
        "\n",
        "        # Save the plot to the file\n",
        "        plt.savefig(output_file)\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Plot saved to: {output_file}\")\n",
        "\n",
        "    return\n",
        "\n",
        "def save_vif_analysis(feature_df_No_Corr, path):\n",
        "    file_path = os.path.join(path, 'VIF')\n",
        "\n",
        "    # Loop over each DataFrame in the list\n",
        "    for i, df in enumerate(feature_df_No_Corr, start=1):\n",
        "        # Optionally, select only numeric columns (if needed)\n",
        "        X = sm.add_constant(df)\n",
        "\n",
        "        # Create a DataFrame to hold the VIF results\n",
        "        vif_data = pd.DataFrame()\n",
        "        vif_data[\"Feature\"] = X.columns\n",
        "        vif_data[\"VIF\"] = [variance_inflation_factor(X.values, j)\n",
        "                           for j in range(X.shape[1])]\n",
        "\n",
        "        # Define the output CSV file path\n",
        "        csv_file = os.path.join(file_path, f\"VIF_stock_{i}.csv\")\n",
        "\n",
        "        # Save the VIF results to CSV\n",
        "        vif_data.to_csv(csv_file, index=False)\n",
        "        print(f\"VIF analysis for dataset {i} saved to: {csv_file}\")\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfmVSpJ6pOrO"
      },
      "source": [
        "# Data Extraction\n",
        "In this section, we invoke the *get_features* function to extract the necessary features for the next phase.\n",
        "\n",
        "Our objective is to extract features for both the training and test datasets.\n",
        "\n",
        "As part of the analysis, six datasets are generated for both training and testing, each containing features for:\n",
        "\n",
        "1. The first stock  \n",
        "2. The second stock  \n",
        "3. The third stock  \n",
        "4. The fourth stock  \n",
        "5. The fifth stock  \n",
        "6. All stocks combined\n",
        "\n",
        "\\\n",
        "Thereafter, we convert the data into dataframes.\n",
        "\n",
        "\n",
        "\\\n",
        "**N.B:**\n",
        "*There is no need to normalise the data as it has already been preprocessed*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-A9_p3mpPWG"
      },
      "outputs": [],
      "source": [
        "def get_feature_dfs_data(train_data, test_data):\n",
        "\n",
        "\n",
        "    # Get features for all 5 training stocks and make data frame\n",
        "    features_df_1 = pd.DataFrame(get_features(train_data[0].T))\n",
        "    features_df_2 = pd.DataFrame(get_features(train_data[1].T))\n",
        "    features_df_3 = pd.DataFrame(get_features(train_data[2].T))\n",
        "    features_df_4 = pd.DataFrame(get_features(train_data[3].T))\n",
        "    features_df_5 = pd.DataFrame(get_features(train_data[4].T))\n",
        "    features_df_all = pd.DataFrame(get_features(train_data[5].T))\n",
        "\n",
        "\n",
        "    # Get features for all 5 testing stocks and make data frame\n",
        "    Test_features_df = pd.DataFrame(get_features(test_data[0].T))\n",
        "    Test_features_df_2 = pd.DataFrame(get_features(test_data[1].T))\n",
        "    Test_features_df_3 = pd.DataFrame(get_features(test_data[2].T))\n",
        "    Test_features_df_4 = pd.DataFrame(get_features(test_data[3].T))\n",
        "    Test_features_df_5 = pd.DataFrame(get_features(test_data[4].T))\n",
        "    Test_features_df_all = pd.DataFrame(get_features(test_data[5].T))\n",
        "\n",
        "\n",
        "    # List of feature DataFrames\n",
        "    feature_dfs = [features_df_1, features_df_2, features_df_3, features_df_4, features_df_5, features_df_all]\n",
        "    Test_features_dfs = [Test_features_df, Test_features_df_2, Test_features_df_3, Test_features_df_4, Test_features_df_5, Test_features_df_all]\n",
        "\n",
        "    return feature_dfs, Test_features_dfs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL-uCqwzhNN0"
      },
      "source": [
        "## Clean Data Function\n",
        "\n",
        "Removes zero-variance features (i.e., features that contain only zeros) from both training and testing feature datasets.\n",
        "\n",
        "### Functionality:\n",
        "- Scans the first DataFrame in the training set to identify features with only 0.0 values.\n",
        "- Excludes specific target columns from this check: `'Price Movement t+1'`, `'Price Movement t+5'`, and `'Price Movement t+10'`.\n",
        "- Drops identified zero-variance features from all DataFrames in both the training and testing sets.\n",
        "- Prints the list of removed features for transparency.\n",
        "\n",
        "### Returns:\n",
        "- `feature_dfs_clean`: Cleaned list of training feature DataFrames.\n",
        "- `Test_features_dfs_clean`: Cleaned list of testing feature DataFrames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJtVK8nphM98"
      },
      "outputs": [],
      "source": [
        "def clean_data(features_dfs, Test_features_dfs):\n",
        "      # Feature DataFrames (Replace with your actual DataFrames)\n",
        "      feature_dfs_clean = features_dfs.copy()\n",
        "      Test_features_dfs_clean = Test_features_dfs.copy()\n",
        "\n",
        "      feature_df_clean = features_dfs[0].copy()\n",
        "\n",
        "      # Identify features that contain only 0.0 values\n",
        "\n",
        "      # Exclude specific columns from zero variance check\n",
        "      exclude_columns = ['Price Movement t+1', 'Price Movement t+5', 'Price Movement t+10']\n",
        "\n",
        "      # Identify zero-variance features, excluding specified columns\n",
        "      zero_variance_features = [\n",
        "      col for col in feature_df_clean.columns\n",
        "      if col not in exclude_columns  # Exclude these columns\n",
        "      and feature_df_clean[col].nunique() == 1\n",
        "      and feature_df_clean[col].iloc[0] == 0.0\n",
        "      ]\n",
        "\n",
        "\n",
        "      # Store removed zero-variance features\n",
        "      removed_zero_variance_features = zero_variance_features.copy()\n",
        "\n",
        "      # Apply zero-variance feature removal to lists of feature DataFrames\n",
        "      for df in feature_dfs_clean:\n",
        "          df.drop(columns=zero_variance_features, errors='ignore', inplace=True)\n",
        "\n",
        "      for df in Test_features_dfs_clean:\n",
        "          df.drop(columns=zero_variance_features, errors='ignore', inplace=True)\n",
        "\n",
        "      # Print removed features\n",
        "      print(\"Removed zero-variance features:\", removed_zero_variance_features)\n",
        "\n",
        "      return feature_dfs_clean, Test_features_dfs_clean\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJmOf2BBfD8D"
      },
      "source": [
        "## Correlated Features Removal - `remove_corr_values`\n",
        "\n",
        "Removes highly correlated features from the training and testing datasets based on a specified correlation threshold.\n",
        "\n",
        "### Functionality:\n",
        "- Computes the correlation matrix of the first training DataFrame.\n",
        "- Identifies feature pairs with absolute correlation above `MAX_CORR` (default: 0.7), excluding self-correlations.\n",
        "- Retains one feature from each highly correlated pair and removes the other, unless the feature is one of the essential columns:\n",
        "  `'mid_prices'`, `'Price Movement t+1'`, `'Price Movement t+5'`, `'Price Movement t+10'`.\n",
        "- Applies the same feature removal to both training and testing datasets.\n",
        "\n",
        "### Parameters:\n",
        "- `feature_dfs_clean`: List of cleaned training DataFrames.\n",
        "- `Test_features_dfs_clean`: List of cleaned testing DataFrames.\n",
        "- `MAX_CORR`: Maximum allowed correlation between features (default is 0.7).\n",
        "\n",
        "### Returns:\n",
        "- `feature_dfs_No_Corr`: List of training DataFrames with reduced multicollinearity.\n",
        "- `Test_features_dfs_No_Corr`: List of testing DataFrames with correlated features removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffZsKoSyfMWL"
      },
      "outputs": [],
      "source": [
        "def remove_corr_values(feature_dfs_clean, Test_features_dfs_clean, MAX_CORR = 0.7):\n",
        "\n",
        "    feature_df_clean = feature_dfs_clean[0].copy()\n",
        "\n",
        "    # Set of features to always keep, even if highly correlated\n",
        "    features_to_keep = {'mid_prices', 'Price Movement t+1', 'Price Movement t+5', 'Price Movement t+10'}\n",
        "\n",
        "    # Compute the correlation matrix\n",
        "    corr_matrix = feature_df_clean.corr()\n",
        "\n",
        "    # Find feature pairs with high correlation (excluding self-correlation)\n",
        "    high_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y]) for x, y in zip(*\n",
        "        (np.where((np.abs(corr_matrix) >= MAX_CORR) & (corr_matrix != 1.0))))]\n",
        "\n",
        "    # Identify highly correlated features to remove, ensuring one feature remains from each pair\n",
        "    features_to_remove = set()\n",
        "    seen_features = set()\n",
        "\n",
        "    for feature_x, feature_y in high_corr_pairs:\n",
        "        if feature_x in features_to_keep or feature_y in features_to_keep:\n",
        "            continue  # Always keep the essential features\n",
        "\n",
        "        if feature_x not in seen_features:\n",
        "            features_to_remove.add(feature_y)  # Keep feature_x, remove feature_y\n",
        "            seen_features.add(feature_x)\n",
        "        elif feature_y not in seen_features:\n",
        "            features_to_remove.add(feature_x)  # Keep feature_y, remove feature_x\n",
        "            seen_features.add(feature_y)\n",
        "\n",
        "    # Store removed highly correlated features\n",
        "    removed_high_corr_features = list(features_to_remove)\n",
        "\n",
        "    # Drop highly correlated features from the dataset\n",
        "    feature_df_No_Corr = feature_df_clean.drop(columns=removed_high_corr_features, errors='ignore')\n",
        "\n",
        "    feature_dfs_No_Corr = feature_dfs_clean.copy()\n",
        "    Test_features_dfs_No_Corr = Test_features_dfs_clean.copy()\n",
        "\n",
        "    for df in feature_dfs_No_Corr:\n",
        "        df.drop(columns=removed_high_corr_features, errors='ignore', inplace=True)\n",
        "\n",
        "    for df in Test_features_dfs_No_Corr:\n",
        "        df.drop(columns=removed_high_corr_features, errors='ignore', inplace=True)\n",
        "\n",
        "    return feature_dfs_No_Corr, Test_features_dfs_No_Corr\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjj4j6jw2ypW"
      },
      "source": [
        "## Group data to prepare to send to model `group_data_dfs`\n",
        "\n",
        "Generates windowed datasets from input feature and movement DataFrames for use in time-series modeling.\n",
        "\n",
        "### Functionality:\n",
        "- Splits each DataFrame in `dfs` into overlapping windows of size `window`, with a step size defined by `displacement`.\n",
        "- Each window contains sequences of feature values, preserving temporal order.\n",
        "- Appends future movement labels from `movement_dfs` based on the last index in each window, for all columns that start with `'Price Movement'`.\n",
        "\n",
        "### Parameters:\n",
        "- `dfs`: List of feature DataFrames (one per asset or stock).\n",
        "- `movement_dfs`: Corresponding list of movement DataFrames with target labels.\n",
        "- `window`: Number of timesteps in each rolling window (default: 10).\n",
        "- `displacement`: Step size between consecutive windows (default: 1).\n",
        "- `threshold`: Optional threshold for movement (not used in this implementation but may be relevant for later classification).\n",
        "\n",
        "### Returns:\n",
        "- `all_dfs`: List of windowed DataFrames, each containing sequences of features and associated movement labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HA_RW66w7TgX"
      },
      "outputs": [],
      "source": [
        "def group_data_dfs(dfs, movement_dfs, window=10, displacement=1, threshold=0.001):\n",
        "\n",
        "    all_dfs = []\n",
        "\n",
        "    # Process each dataset and its corresponding movement DataFrame.\n",
        "    for idx, df in enumerate(dfs):\n",
        "        L = len(df)\n",
        "        if L < window:\n",
        "            all_dfs.append(pd.DataFrame())\n",
        "            continue\n",
        "\n",
        "        new_data = {}\n",
        "\n",
        "        # Generate windows for each column in the feature DataFrame.\n",
        "        for col in df.columns:\n",
        "            windows_list = []\n",
        "            i = 0\n",
        "            while (i * displacement + window) <= L:\n",
        "                w = df[col].iloc[i * displacement : i * displacement + window].tolist()\n",
        "                windows_list.append(w)\n",
        "                i += 1\n",
        "            new_data[col] = windows_list\n",
        "\n",
        "        num_windows = i  # Total number of windows generated\n",
        "\n",
        "        # For each window, compute the last index using vectorized operations.\n",
        "        last_indices = np.arange(num_windows) * displacement + window - 1\n",
        "\n",
        "         # Automatically extract movement values for each column in mov_df that starts with 'Price Movement'\n",
        "        mov_df = movement_dfs[idx]\n",
        "        movement_cols = [col for col in mov_df.columns if col.startswith('Price Movement')]\n",
        "        for col in movement_cols:\n",
        "            new_data[col] = mov_df.iloc[last_indices][col].tolist()\n",
        "\n",
        "        all_dfs.append(pd.DataFrame(new_data))\n",
        "\n",
        "    return all_dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HeKouMX2ClV"
      },
      "source": [
        "# Compute rolling average to prepare to send to models `group_data_avg_dfs`\n",
        "## Alternative to the previous function\n",
        "\n",
        "Creates windowed datasets where each feature is averaged over rolling windows, combining historical context with target price movement labels.\n",
        "\n",
        "### Functionality:\n",
        "- Slides a window of size `window` across each DataFrame in `dfs` with a step of `displacement`.\n",
        "- Computes the average value of each feature within the window, instead of keeping full sequences.\n",
        "- Extracts corresponding movement labels from `movement_dfs` for all columns beginning with `'Price Movement'`.\n",
        "\n",
        "### Parameters:\n",
        "- `dfs`: List of feature DataFrames.\n",
        "- `movement_dfs`: List of DataFrames containing movement labels.\n",
        "- `window`: Number of timesteps in each rolling window (default: 10).\n",
        "- `displacement`: Step size between windows (default: 2).\n",
        "- `threshold`: Threshold placeholder for potential filtering (unused in current implementation).\n",
        "\n",
        "### Returns:\n",
        "- `all_dfs`: List of DataFrames where each row contains the average of windowed features and corresponding movement labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJKW0Chv2B2t"
      },
      "outputs": [],
      "source": [
        "def group_data_avg_dfs(dfs, movement_dfs, window=10, displacement=2, threshold=0.001):\n",
        "\n",
        "    all_dfs = []\n",
        "    for idx, df in enumerate(dfs):\n",
        "        L = len(df)\n",
        "        # We need at least (window) + 1 rows so that the special window and its next value exist.\n",
        "        if L < window + 1:\n",
        "            all_dfs.append(pd.DataFrame())\n",
        "            continue\n",
        "\n",
        "        new_data = {}\n",
        "        # Process every column in the DataFrame.\n",
        "        for col in df.columns:\n",
        "            averages_list = []\n",
        "\n",
        "            # Build subsequent windows using the displacement value.\n",
        "            i = 0\n",
        "            while (i * displacement + window) < L:\n",
        "                w = df[col].iloc[i * displacement : i * displacement + window].tolist()\n",
        "                averages_list.append(sum(w) / len(w))  # Compute average of the window\n",
        "                i += 1\n",
        "\n",
        "            new_data[col] = averages_list\n",
        "\n",
        "        # Get the movement columns\n",
        "        num_windows = i  # Total number of windows generated\n",
        "\n",
        "        # For each window, compute the last index using vectorized operations.\n",
        "        last_indices = np.arange(num_windows) * displacement + window - 1\n",
        "\n",
        "         # Automatically extract movement values for each column in mov_df that starts with 'Price Movement'\n",
        "        mov_df = movement_dfs[idx]\n",
        "        movement_cols = [col for col in mov_df.columns if col.startswith('Price Movement')]\n",
        "        for col in movement_cols:\n",
        "            new_data[col] = mov_df.iloc[last_indices][col].tolist()\n",
        "\n",
        "        all_dfs.append(pd.DataFrame(new_data))\n",
        "\n",
        "    return all_dfs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x6G57Qw_PNA"
      },
      "source": [
        "## Prepare Data for Loop `prepare_data_for_loop`\n",
        "### Larger function that call the previous functions\n",
        "\n",
        "Prepares training and testing datasets for model input through a full preprocessing pipeline.\n",
        "\n",
        "### Functionality:\n",
        "- Splits raw training and testing data by stock.\n",
        "- Extracts and separates future price movement targets (`t+1`, `t+5`, `t+10`).\n",
        "- Cleans features by removing zero-variance columns and highly correlated features.\n",
        "- Constructs averaged rolling window features for model input.\n",
        "- Optionally includes data visualization (commented out).\n",
        "- Returns full and de-correlated feature sets in both raw and windowed form.\n",
        "\n",
        "### Returns:\n",
        "- `train_group_dfs_clean`, `test_group_dfs_clean`: Rolling window averaged training and testing sets.\n",
        "- `train_group_dfs_no_corr`, `test_group_dfs_no_corr`: Same as above, but with correlated features removed.\n",
        "- `train_dfs_clean`, `test_dfs_clean`: Cleaned base feature DataFrames (no zero-variance features).\n",
        "- `train_dfs_no_corr`, `test_dfs_no_corr`: Cleaned DataFrames with correlated features removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFyXdfoKNdnn"
      },
      "outputs": [],
      "source": [
        "def get_reduced_df(df_list, VALUE_PERCENTAGE = 0.05):\n",
        "\n",
        "    reduced_list = []\n",
        "\n",
        "    for df in df_list:\n",
        "        num_rows = max(1, int(len(df) * VALUE_PERCENTAGE))  # Ensure at least 1 row is kept\n",
        "        reduced_df = df.iloc[:num_rows]  # Take the first num_rows rows\n",
        "        reduced_list.append(reduced_df)\n",
        "\n",
        "    return reduced_list\n",
        "\n",
        "# Set window for rolling comulative sum\n",
        "WINDOW = 30\n",
        "\n",
        "def prepare_data_for_loop(train_data, test_data, preprocessing_path):\n",
        "\n",
        "      # Send csv data that returns 2x array (train and test) of 6: one per stock and one for combination of all stocks\n",
        "      train_stocks_split = get_stocks_old(train_data) #\n",
        "      test_stocks_split = get_stocks_old(test_data) #\n",
        "\n",
        "      print('\\nData split processing was successful\\n')\n",
        "\n",
        "      # Get the features for each the training and testing data - Still an array of 6\n",
        "      train_dfs_stocks, test_dfs_stocks = get_feature_dfs_data(train_stocks_split, test_stocks_split) #2\n",
        "\n",
        "      print('\\nFeatures were successfully extracted\\n')\n",
        "\n",
        "      # Extract 'Price Movement' columns from train and test datasets\n",
        "      price_movement_train = [df[['Price Movement t+1', 'Price Movement t+5', 'Price Movement t+10']] for df in train_dfs_stocks]\n",
        "      price_movement_test = [df[['Price Movement t+1', 'Price Movement t+5', 'Price Movement t+10']] for df in test_dfs_stocks]\n",
        "\n",
        "      # Remove the extracted columns from the original datasets\n",
        "      train_dfs_stocks = [df.drop(['Price Movement t+1', 'Price Movement t+5', 'Price Movement t+10'], axis=1) for df in train_dfs_stocks]\n",
        "      test_dfs_stocks = [df.drop(['Price Movement t+1', 'Price Movement t+5', 'Price Movement t+10'], axis=1) for df in test_dfs_stocks]\n",
        "\n",
        "      print('\\nPrice Movement Extracted for t+1, t+5 and t+10\\n')\n",
        "\n",
        "      # # Clean the features data frame by removing features containing majoratively 0.0\n",
        "      train_dfs_clean, test_dfs_clean = clean_data(train_dfs_stocks, test_dfs_stocks) #3\n",
        "\n",
        "      # # Removes highly correlated features (keeps one of the pair)\n",
        "      train_dfs_no_corr, test_dfs_no_corr = remove_corr_values(train_dfs_clean, test_dfs_clean) #4\n",
        "\n",
        "      # Get window of information in data frames\n",
        "      train_group_dfs_clean = group_data_avg_dfs(train_dfs_clean, price_movement_train)\n",
        "      test_group_dfs_clean = group_data_avg_dfs(test_dfs_clean, price_movement_test)\n",
        "      train_group_dfs_no_corr = group_data_avg_dfs(train_dfs_no_corr, price_movement_train)\n",
        "      test_group_dfs_no_corr = group_data_avg_dfs(test_dfs_no_corr, price_movement_test)\n",
        "\n",
        "      print('\\nExtracted low correlation features sucessfully\\n')\n",
        "\n",
        "      # Plots\n",
        "    #   plot_stocks(train_dfs_stocks, preprocessing_path)\n",
        "    #   plot_corr_matrix(train_dfs_clean, preprocessing_path, True)\n",
        "    #   plot_corr_matrix(train_dfs_no_corr, preprocessing_path)\n",
        "    #   save_vif_analysis(train_dfs_stocks, preprocessing_path)\n",
        "\n",
        "      print('\\nPreprocessing successful\\n')\n",
        "\n",
        "      # TODO return only 5% of the data\n",
        "      return train_group_dfs_clean, test_group_dfs_clean, train_group_dfs_no_corr, test_group_dfs_no_corr, train_dfs_clean, test_dfs_clean, train_dfs_no_corr, test_dfs_no_corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znYkvceC_PNA"
      },
      "source": [
        "# **Part 2: Forecasting Mid-Price Movements**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L9NI5lEPNlv"
      },
      "source": [
        "## Random Forest\n",
        "\n",
        "Trains and evaluates a Random Forest classifier on windowed time-series data for multi-class mid-price movement prediction. Outputs performance metrics and visual comparison.\n",
        "\n",
        "### Functionality:\n",
        "- Preprocesses the input data by flattening windowed features and scaling them.\n",
        "- Trains a `RandomForestClassifier` with 100 trees.\n",
        "- Predicts movement classes for the test set.\n",
        "- Evaluates model performance using Accuracy, F1 score (weighted), Cohen’s Kappa, and Precision.\n",
        "- Saves metrics to a CSV file and plots the actual vs. predicted labels.\n",
        "\n",
        "### Parameters:\n",
        "- `train`: Training DataFrame with averaged or windowed features and a movement label.\n",
        "- `test`: Testing DataFrame in the same format.\n",
        "- `csv_file_path`: File path to append model evaluation results.\n",
        "- `path_figures`: Folder where prediction plots will be saved.\n",
        "- `tnn`: Identifier (e.g. label or time horizon name) written to the CSV for clarity.\n",
        "\n",
        "### Returns:\n",
        "- A list containing:\n",
        "  - The trained Random Forest model.\n",
        "  - Scaled training features (`X_train_scaled`).\n",
        "  - Scaled test features (`X_test_scaled`).\n",
        "  - Feature names from the original DataFrame (`train.columns`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuDPG3nuPNlw"
      },
      "outputs": [],
      "source": [
        "def run_random_forest(train, test, csv_file_path, path_figures, tnn):\n",
        "    def prepare_data(df):\n",
        "        X = np.stack([np.stack(row) for row in df.iloc[:, :-1].values], axis=0)\n",
        "        y = df.iloc[:, -1].values.astype(int)\n",
        "        return X, y\n",
        "\n",
        "    X_train, y_train = prepare_data(train)\n",
        "    X_test, y_test = prepare_data(test)\n",
        "\n",
        "    # Reshape to (samples, timesteps * features) for Random Forest\n",
        "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_flat)\n",
        "    X_test_scaled = scaler.transform(X_test_flat)\n",
        "\n",
        "    # Random Forest model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    predictions_class = model.predict(X_test_scaled)\n",
        "\n",
        "    # Metrics calculation\n",
        "    accuracy = accuracy_score(y_test, predictions_class)\n",
        "    f1 = f1_score(y_test, predictions_class, average='weighted')\n",
        "    kappa = cohen_kappa_score(y_test, predictions_class)\n",
        "    precision = precision_score(y_test, predictions_class, average='weighted')\n",
        "\n",
        "    # Write metrics to CSV\n",
        "    with open(csv_file_path, 'a') as file:\n",
        "        file.write(f'Random_Forest {tnn},{accuracy},{f1},{kappa},{precision}\\n')\n",
        "\n",
        "    # Plotting results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(y_test, label='Actual', linestyle='dashed')\n",
        "    plt.plot(predictions_class, label='Predicted', linestyle='solid')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Class')\n",
        "    plt.title('Random Forest Predictions vs Actual Classes')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(path_figures, 'random_forest_classification.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return [model, X_train_scaled, X_test_scaled, train.columns]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IkEQ7fQPNlx"
      },
      "source": [
        "## LSTMClassifier Pipeline\n",
        "\n",
        "Implements and evaluates an LSTM-based deep learning model for multi-class classification using time-series feature data from high-frequency trading.\n",
        "\n",
        "\n",
        "\n",
        "### prepare_data\n",
        "\n",
        "Preprocesses a windowed DataFrame into 3D format suitable for LSTM input.\n",
        "\n",
        "#### Output:\n",
        "- `X`: 3D NumPy array with shape `(samples, timesteps, features)`\n",
        "- `y`: Class label array (integer encoded)\n",
        "\n",
        "\n",
        "\n",
        "### LSTMClassifier\n",
        "\n",
        "Custom TensorFlow model with:\n",
        "- Two stacked LSTM layers (`return_sequences=True`)\n",
        "- Dropout layers for regularization\n",
        "- Dense layers for classification output (logits, no softmax)\n",
        "\n",
        "\n",
        "\n",
        "### run_lstm\n",
        "\n",
        "Trains and evaluates the LSTM model, saving metrics and prediction plots.\n",
        "\n",
        "#### Functionality:\n",
        "- Prepares data and ensures proper input shape\n",
        "- Applies feature scaling using `StandardScaler`\n",
        "- Compiles and trains the LSTM model\n",
        "- Evaluates model with:\n",
        "  - Accuracy\n",
        "  - Weighted F1 Score\n",
        "  - Cohen’s Kappa\n",
        "  - Weighted Precision\n",
        "- Appends results to a CSV file\n",
        "- Saves a plot comparing actual and predicted class labels\n",
        "\n",
        "#### Parameters:\n",
        "- `train`, `test`: Preprocessed DataFrames with time-series features and target class\n",
        "- `csv_file_path`: Path to output metrics\n",
        "- `path_figures`: Directory to save the plot\n",
        "- `tnn`: Identifier for the time horizon or run\n",
        "- `epochs`: Number of training epochs (default: 20)\n",
        "- `batch_size`: Batch size for training (default: 32)\n",
        "\n",
        "#### Returns:\n",
        "- List containing the trained model, training and test feature arrays, and column names\n",
        "\n",
        "\n",
        "Code Inpired by Charrejee et al. (2021): [Link article](https://arxiv.org/pdf/2111.01137)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4p9UGBH_PNB"
      },
      "outputs": [],
      "source": [
        "# Data preparation: stack features and ensure 3D shape: (samples, timesteps, features)\n",
        "def prepare_data(df):\n",
        "    X = np.stack([np.array(row) for row in df.iloc[:, :-1].values], axis=0)\n",
        "    y = df.iloc[:, -1].values.astype(int)\n",
        "\n",
        "    # Ensure X has 3 dimensions: (samples, timesteps, features)\n",
        "    if X.ndim == 2:\n",
        "        X = np.expand_dims(X, axis=-1)\n",
        "    return X, y\n",
        "\n",
        "# LSTMClassifier implemented in TensorFlow\n",
        "class LSTMClassifier(tf.keras.Model):\n",
        "    def __init__(self, input_size, hidden_size1=256, hidden_size2=128, output_size=3):\n",
        "        \"\"\"\n",
        "        input_size: number of features per timestep\n",
        "        hidden_size1: number of units for the first LSTM layer\n",
        "        hidden_size2: number of units for the second LSTM layer\n",
        "        output_size: number of output classes\n",
        "        \"\"\"\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        # First LSTM layer; return sequences for further processing\n",
        "        self.lstm1 = tf.keras.layers.LSTM(hidden_size1, return_sequences=True)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(0.2)\n",
        "        # Second LSTM layer; we use return_sequences=True so we can extract the last time step explicitly\n",
        "        self.lstm2 = tf.keras.layers.LSTM(hidden_size2, return_sequences=True)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(0.2)\n",
        "        self.fc1 = tf.keras.layers.Dense(8, activation='relu')\n",
        "        self.fc2 = tf.keras.layers.Dense(output_size)  # Logits; no activation here\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.lstm1(x, training=training)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.lstm2(x, training=training)\n",
        "        # Extract the output at the last timestep\n",
        "        x = x[:, -1, :]\n",
        "        x = self.dropout2(x, training=training)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Training and evaluation function\n",
        "def run_lstm(train, test, csv_file_path, path_figures, tnn, epochs=20, batch_size=32):\n",
        "    # Prepare data from pandas DataFrames\n",
        "    X_train, y_train = prepare_data(train)\n",
        "    X_test, y_test = prepare_data(test)\n",
        "    X_columns = train.columns[:-1]\n",
        "\n",
        "    # Ensure correct shape: if the second dimension is larger than the third, transpose.\n",
        "    if X_train.shape[1] > X_train.shape[2]:\n",
        "        X_train = np.transpose(X_train, (0, 2, 1))\n",
        "        X_test = np.transpose(X_test, (0, 2, 1))\n",
        "\n",
        "    # Scale features (flatten the time dimension for scaling and reshape back)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "    X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
        "\n",
        "    # Build the model. The input_size is the number of features per timestep.\n",
        "    model = LSTMClassifier(input_size=X_train_scaled.shape[2])\n",
        "\n",
        "    # Compile the model with Adam optimizer and sparse categorical crossentropy loss (from logits)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model using model.fit with validation on the test set\n",
        "    history = model.fit(X_train_scaled, y_train,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=(X_test_scaled, y_test),\n",
        "                        verbose=2)\n",
        "\n",
        "    # Obtain predictions on the test set\n",
        "    logits = model.predict(X_test_scaled)\n",
        "    predictions_class = np.argmax(logits, axis=1)\n",
        "\n",
        "    # Calculate evaluation metrics using scikit-learn\n",
        "    accuracy = accuracy_score(y_test, predictions_class)\n",
        "    f1 = f1_score(y_test, predictions_class, average='weighted')\n",
        "    kappa = cohen_kappa_score(y_test, predictions_class)\n",
        "    precision = precision_score(y_test, predictions_class, average='weighted')\n",
        "\n",
        "    # Append the metrics to the CSV file\n",
        "    with open(csv_file_path, 'a') as file:\n",
        "        file.write(f'LSTM {tnn},{accuracy},{f1},{kappa},{precision}\\n')\n",
        "\n",
        "    # Plot the actual vs. predicted classes\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(y_test, label='Actual', linestyle='dashed')\n",
        "    plt.plot(predictions_class, label='Predicted', linestyle='solid')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Class')\n",
        "    plt.title('LSTM Predictions vs Actual Classes')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(path_figures, 'lstm_classification.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return [model, X_train, X_test, X_columns]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKaEEmIDPNly"
      },
      "source": [
        "## ResNet-LSTM Classifier\n",
        "\n",
        "Implements a hybrid architecture combining residual convolutional layers (ResNet) with LSTM layers for enhanced sequence modeling. Designed for mid-price movement prediction from time-series limit order book features.\n",
        "\n",
        "\n",
        "### ResNetLSTMClassifier (TensorFlow Model)\n",
        "\n",
        "A custom deep learning model that:\n",
        "- Applies **1D convolutional layers** with residual connections to extract robust local patterns across time.\n",
        "- Passes extracted features through stacked **LSTM layers** to capture temporal dependencies.\n",
        "- Ends with dense layers for multi-class classification output (via logits).\n",
        "\n",
        "#### Architecture Overview:\n",
        "- Conv1D → BatchNorm → ReLU\n",
        "- Residual block(s) with skip connections\n",
        "- LSTM layers (with `return_sequences`)\n",
        "- Dropout + Dense layers\n",
        "- Final dense layer without activation (logits)\n",
        "\n",
        "\n",
        "### run_resnet_lstm\n",
        "\n",
        "Trains and evaluates the ResNet-LSTM model using windowed and scaled time-series input.\n",
        "\n",
        "#### Functionality:\n",
        "- Prepares the 3D feature data `(samples, timesteps, features)`\n",
        "- Applies feature scaling\n",
        "- Compiles the model with Adam optimizer and sparse categorical crossentropy loss\n",
        "- Trains the model with early stopping or fixed epochs\n",
        "- Evaluates performance using:\n",
        "  - Accuracy\n",
        "  - Weighted F1 score\n",
        "  - Cohen’s Kappa\n",
        "  - Weighted Precision\n",
        "- Saves evaluation metrics to CSV and comparison plots to PNG\n",
        "\n",
        "#### Parameters:\n",
        "- `train`, `test`: DataFrames with 3D windowed features and class labels\n",
        "- `csv_file_path`: Output path for saving metrics\n",
        "- `path_figures`: Directory to save prediction plot\n",
        "- `tnn`: Run label (e.g., time horizon)\n",
        "- `epochs`: Training epochs (default: 20)\n",
        "- `batch_size`: Training batch size (default: 32)\n",
        "\n",
        "#### Returns:\n",
        "- Trained model\n",
        "- Scaled train and test features\n",
        "- Feature column names from the original data\n",
        "\n",
        "Code Inpired by Jia et al. (2025): [Link article](https://arxiv.org/pdf/2312.01020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXjJyGL9_PNB"
      },
      "outputs": [],
      "source": [
        "# Custom model equivalent to the PyTorch ResNLS\n",
        "class ResNLS(tf.keras.Model):\n",
        "    def __init__(self, input_dim, hidden_dim=64, output_dim=3):\n",
        "        \"\"\"\n",
        "        input_dim: number of time steps (or original feature length)\n",
        "        hidden_dim: number of filters/hidden units in CNN and LSTM\n",
        "        output_dim: number of classes for classification\n",
        "        \"\"\"\n",
        "        super(ResNLS, self).__init__()\n",
        "        # Create a trainable scalar weight parameter\n",
        "        self.weight = self.add_weight(name=\"weight\",\n",
        "                                      shape=(1,),\n",
        "                                      initializer=tf.zeros_initializer(),\n",
        "                                      trainable=True)\n",
        "\n",
        "        # CNN Block\n",
        "        self.conv1 = tf.keras.layers.Conv1D(filters=hidden_dim,\n",
        "                                            kernel_size=3,\n",
        "                                            strides=1,\n",
        "                                            padding='same',\n",
        "                                            activation='relu')\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization(epsilon=1e-5)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.conv2 = tf.keras.layers.Conv1D(filters=hidden_dim,\n",
        "                                            kernel_size=3,\n",
        "                                            strides=1,\n",
        "                                            padding='same',\n",
        "                                            activation='relu')\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization(epsilon=1e-5)\n",
        "        # Global average pooling over the time dimension (equivalent to AdaptiveAvgPool1d(1))\n",
        "        self.global_pool = tf.keras.layers.GlobalAveragePooling1D()\n",
        "        # Fully-connected layer to map CNN output back to input dimension (time steps)\n",
        "        self.fc_cnn = tf.keras.layers.Dense(input_dim)\n",
        "\n",
        "        # LSTM Block: return_state=True so we can use the last hidden state\n",
        "        self.lstm = tf.keras.layers.LSTM(hidden_dim, return_state=True)\n",
        "        self.linear = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        # x is expected to be of shape (batch, sequence_length, channels)\n",
        "        # CNN Block\n",
        "        cnn = self.conv1(x)\n",
        "        cnn = self.bn1(cnn, training=training)\n",
        "        cnn = self.dropout(cnn, training=training)\n",
        "        cnn = self.conv2(cnn)\n",
        "        cnn = self.bn2(cnn, training=training)\n",
        "        cnn = self.global_pool(cnn)  # shape becomes (batch, hidden_dim)\n",
        "        cnn = self.fc_cnn(cnn)       # shape becomes (batch, input_dim)\n",
        "        # Expand dimensions to create a \"channel\" dimension, now (batch, input_dim, 1)\n",
        "        cnn = tf.expand_dims(cnn, axis=-1)\n",
        "\n",
        "        # Residual connection: add original input and weighted CNN output.\n",
        "        # Assumes that the original input x has one channel and sequence length equals input_dim.\n",
        "        residuals = x + self.weight * cnn\n",
        "\n",
        "        # LSTM Block: process the residual sequence.\n",
        "        # Only the final hidden state is used for classification.\n",
        "        _, state_h, _ = self.lstm(residuals)\n",
        "        y_hat = self.linear(state_h)\n",
        "        return y_hat\n",
        "\n",
        "# Data preparation function adjusted for TensorFlow (channels-last)\n",
        "def prepare_data(df):\n",
        "    # Stack feature columns; assume last column is the target\n",
        "    X = np.stack(df.iloc[:, :-1].values, axis=0)\n",
        "    y = df.iloc[:, -1].values.astype(int)\n",
        "    # Reshape to (batch, sequence_length, channels); here channels=1\n",
        "    return X.reshape(X.shape[0], X.shape[1], 1), y\n",
        "\n",
        "# Training and evaluation function\n",
        "def run_resnet(train, test, csv_file_path, path_figures, tnn, epochs=50, batch_size=64):\n",
        "    # Get column names (for potential future use, e.g., in SHAP analysis)\n",
        "    X_train_columns = train.columns\n",
        "    X_train, y_train = prepare_data(train)\n",
        "    X_test, y_test = prepare_data(test)\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler_X = StandardScaler()\n",
        "    X_train = scaler_X.fit_transform(X_train.reshape(X_train.shape[0], -1)).reshape(X_train.shape)\n",
        "    X_test = scaler_X.transform(X_test.reshape(X_test.shape[0], -1)).reshape(X_test.shape)\n",
        "\n",
        "    print(\"Unique class labels in train:\", np.unique(y_train))\n",
        "    print(\"Unique class labels in test:\", np.unique(y_test))\n",
        "\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    # Instantiate the model.\n",
        "    # Here, input_dim corresponds to the sequence length (number of features)\n",
        "    model = ResNLS(input_dim=X_train.shape[1], output_dim=num_classes)\n",
        "\n",
        "    # Compile the model with Adam optimizer and sparse categorical crossentropy loss.\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "    # Train the model using model.fit, including validation on test data.\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(X_test, y_test),\n",
        "                        verbose=2)\n",
        "\n",
        "    # Evaluate predictions on test set.\n",
        "    logits = model(X_test, training=False)\n",
        "    predictions = tf.argmax(logits, axis=1).numpy()\n",
        "    y_test_np = y_test\n",
        "\n",
        "    # Compute classification metrics using scikit-learn.\n",
        "    accuracy = accuracy_score(y_test_np, predictions)\n",
        "    f1 = f1_score(y_test_np, predictions, average='weighted')\n",
        "    kappa = cohen_kappa_score(y_test_np, predictions)\n",
        "    precision = precision_score(y_test_np, predictions, average='weighted')\n",
        "\n",
        "    # Append results to the CSV file.\n",
        "    with open(csv_file_path, 'a') as file:\n",
        "        file.write(f'ResNet {tnn},{accuracy},{f1},{kappa},{precision}\\n')\n",
        "\n",
        "    # Plot actual vs. predicted class labels.\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(y_test_np, label='Actual', linestyle='dashed')\n",
        "    plt.plot(predictions, label='Predicted', linestyle='solid')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Class')\n",
        "    plt.title('ResNet Classification Predictions vs Actual Classes')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(path_figures, 'resnet_classification.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print('SHAP CALLING')\n",
        "    # shap(model, X_train, X_test, X_train_columns)\n",
        "    print('SHAP DONE')\n",
        "\n",
        "    return [model, X_train, X_test, X_train_columns]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-9VJypAPNlz"
      },
      "source": [
        "## ARIMAX\n",
        "\n",
        "Fits and evaluates an ARIMAX (AutoRegressive Integrated Moving Average with Exogenous inputs) model for time-series regression using both autoregressive and feature-driven components.\n",
        "\n",
        "### Functionality:\n",
        "- Splits the dataset into target (`y`) and exogenous variables (`X`).\n",
        "- Scales the exogenous variables using `StandardScaler`.\n",
        "- Fits an ARIMAX model on the training set using the specified order (default: (1, 1, 1)).\n",
        "- Predicts values on the test set using the scaled exogenous features.\n",
        "- Computes both regression and classification metrics:\n",
        "  - Accuracy (based on rounded class predictions)\n",
        "  - Mean Squared Error (MSE)\n",
        "  - Mean Absolute Error (MAE)\n",
        "  - R-squared (R²)\n",
        "- Appends results to a CSV file and saves a plot comparing predicted and actual values.\n",
        "\n",
        "### Parameters:\n",
        "- `train`, `test`: DataFrames where the first column is the target and the remaining are features.\n",
        "- `csv_file_path`: Path to append evaluation metrics.\n",
        "- `path_figures`: Directory to save prediction plot.\n",
        "- `tnn`: Identifier for model tracking (e.g. time horizon).\n",
        "- `order`: ARIMA order tuple (p, d, q) (default: (1, 1, 1)).\n",
        "\n",
        "### Returns:\n",
        "- List containing:\n",
        "  - Trained SARIMAX model\n",
        "  - Scaled training and testing features\n",
        "  - Raw training and testing target arrays\n",
        "  - Feature column names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XWW2xwHPNlz"
      },
      "outputs": [],
      "source": [
        "def run_arimax(train, test, csv_file_path, path_figures, tnn, order=(1, 1, 1)):\n",
        "    warnings.filterwarnings(\"ignore\")  # Suppress convergence warnings\n",
        "\n",
        "    # Split into target and features\n",
        "    y_train, X_train = train.iloc[:, 0], train.iloc[:, 1:]\n",
        "    y_test, X_test = test.iloc[:, 0], test.iloc[:, 1:]\n",
        "\n",
        "    # Scale exogenous variables\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Fit ARIMAX model with simpler order to reduce overfitting\n",
        "    model = SARIMAX(endog=y_train, exog=X_train_scaled, order=order,\n",
        "                    enforce_stationarity=True, enforce_invertibility=True)\n",
        "    model_fit = model.fit(disp=False, maxiter=300, method='powell')\n",
        "\n",
        "    # Forecast\n",
        "    predictions = model_fit.predict(start=len(y_train), end=len(y_train) + len(y_test) - 1, exog=X_test_scaled)\n",
        "\n",
        "    # Round for classification-style accuracy\n",
        "    y_pred_classes = np.round(predictions).astype(int)\n",
        "    y_true_classes = np.round(y_test).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    metrics = [accuracy, mse, mae, r2]\n",
        "\n",
        "    # Log metrics\n",
        "    with open(csv_file_path, 'a') as file:\n",
        "        file.write(f'arimax{tnn},{\" ,\".join(map(str, metrics))}\\n')\n",
        "\n",
        "    # Plot predictions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(y_test.values, label='Actual', linestyle='dashed')\n",
        "    plt.plot(predictions, label='Predicted', linestyle='solid')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Target')\n",
        "    plt.title(f'ARIMAX Predictions vs Actual (Model {tnn})')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(path_figures, f'arimax_prediction_{tnn}.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return [model_fit, X_train, X_test, y_train, y_test, X_train.columns]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs9kFd01_PNC"
      },
      "source": [
        "# Function to get Absolute Mean SHAP Values\n",
        "It is recommended to run a specific model and with a specific time horizong as they take a lot of computing power and time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm8bJlJ6_PNC"
      },
      "outputs": [],
      "source": [
        "def get_shap_arimax(model_fit, X_train, X_test, y_train, y_test, feature_names, file_path, order=(1, 1, 1), z=1, background_size=50, EXPLAIN=2, ROUNDING=4):\n",
        "    # Create a scaler and fit on the training exogenous data (unscaled)\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "\n",
        "    # Define a prediction wrapper function that maps unscaled exogenous features to predictions.\n",
        "    def arimax_predict_wrapper(X):\n",
        "        # X is expected to be a 2D numpy array (samples, features)\n",
        "        preds = []\n",
        "        for i in range(X.shape[0]):\n",
        "            x_row = X[i].reshape(1, -1)\n",
        "            # Scale the input row using the already fitted scaler.\n",
        "            x_scaled = scaler.transform(x_row)\n",
        "            # Forecast one step ahead using the ARIMAX model.\n",
        "            pred = model_fit.forecast(steps=1, exog=x_scaled)\n",
        "            # --- Amendment: Use .iloc[0] instead of pred[0] to extract the first value\n",
        "            preds.append(pred.iloc[0])\n",
        "            # --- End Amendment\n",
        "        return np.array(preds)\n",
        "\n",
        "    # Prepare background data from the training exogenous features.\n",
        "    X_train_array = X_train.values\n",
        "    if X_train_array.shape[0] > background_size:\n",
        "        indices = np.random.choice(X_train_array.shape[0], background_size, replace=False)\n",
        "        background_data = X_train_array[indices]\n",
        "    else:\n",
        "        background_data = X_train_array\n",
        "\n",
        "    # Initialize SHAP KernelExplainer using the ARIMAX prediction wrapper.\n",
        "    explainer = shap.KernelExplainer(arimax_predict_wrapper, background_data, feature_names=list(feature_names))\n",
        "\n",
        "    # Compute SHAP values for the first EXPLAIN samples from the test set.\n",
        "    X_test_array = X_test.iloc[:EXPLAIN].values\n",
        "    shap_values = explainer.shap_values(X_test_array)\n",
        "\n",
        "    # Calculate the mean absolute SHAP value for each feature.\n",
        "    mean_shap_abs_values = np.round(np.mean(np.abs(shap_values), axis=0), ROUNDING)\n",
        "\n",
        "    # Create a DataFrame to display feature importance.\n",
        "    df_shap = pd.DataFrame({\n",
        "        \"Feature\": list(feature_names),\n",
        "        \"Average\": mean_shap_abs_values\n",
        "    })\n",
        "    df_shap.sort_values(by=\"Average\", ascending=False, inplace=True)\n",
        "\n",
        "    # Save the SHAP values DataFrame to CSV in the figures folder.\n",
        "    if file_path:\n",
        "        os.makedirs(file_path, exist_ok=True)\n",
        "        df_shap.to_csv(os.path.join(file_path, f'ARIMAX_stock_{z}.csv'), index=False)\n",
        "        print(f\"Saved SHAP values to {file_path}\")\n",
        "    else:\n",
        "        print(\"No file path provided; CSV not saved.\")\n",
        "    return shap_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGznPedf_PNC"
      },
      "outputs": [],
      "source": [
        "def get_shap_random_forest(model, X_train_scaled, X_test_scaled, feature_names, z=1, file_path='', EXPLAIN=20, ROUNDING=4):\n",
        "\n",
        "    # Ensure the feature_names list matches the number of features in X_train_scaled.\n",
        "    n_features = X_train_scaled.shape[1]\n",
        "    if len(feature_names) != n_features:\n",
        "        print(f\"Warning: feature_names length {len(feature_names)} does not match number of features {n_features}. Truncating.\")\n",
        "        feature_names = list(feature_names)[:n_features]\n",
        "\n",
        "    # Initialize TreeExplainer with the trained Random Forest model.\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "\n",
        "    # Compute SHAP values for the first EXPLAIN samples from the test set.\n",
        "    X_test_explain = X_test_scaled[:EXPLAIN]\n",
        "    shap_values = explainer.shap_values(X_test_explain)\n",
        "\n",
        "    # Compute overall mean absolute SHAP importance\n",
        "    if isinstance(shap_values, list):\n",
        "        if len(shap_values) > 1:\n",
        "            # Multi-class: for each class compute mean absolute values (shape: (n_features,))\n",
        "            mean_shap_class = [np.mean(np.abs(sv), axis=0) for sv in shap_values]\n",
        "            mean_shap_class = [np.round(arr, ROUNDING) for arr in mean_shap_class]\n",
        "            # Compute overall average importance by averaging across classes\n",
        "            overall_importance = np.round(np.mean(mean_shap_class, axis=0), ROUNDING)\n",
        "        else:\n",
        "            overall_importance = np.round(np.mean(np.abs(shap_values[0]), axis=0), ROUNDING)\n",
        "    else:\n",
        "        overall_importance = np.round(np.mean(np.abs(shap_values), axis=0), ROUNDING)\n",
        "\n",
        "    # Ensure overall_importance is 1D\n",
        "    overall_importance = np.asarray(overall_importance).flatten()\n",
        "\n",
        "    # Ensure feature_names is a list with the same length as the number of features\n",
        "    n_features = overall_importance.shape[0]\n",
        "    if len(feature_names) != n_features:\n",
        "        feature_names = list(feature_names)[:n_features]\n",
        "\n",
        "    mean_shap_abs_values = np.round(np.abs(shap_values).mean(axis=0), ROUNDING)\n",
        "    shap_avg = np.mean(mean_shap_abs_values, axis=1)\n",
        "\n",
        "\n",
        "    # Multi-class: assume 3 classes and create a DataFrame with per-class and overall averages.\n",
        "    # mean_shap_class = [np.round(np.mean(np.abs(sv), axis=0), ROUNDING).flatten() for sv in shap_values]\n",
        "    # print(mean_shap_class)\n",
        "\n",
        "    df_shap = pd.DataFrame({\n",
        "        \"Feature\": feature_names,\n",
        "        \"Increase\": mean_shap_abs_values[:, 0],\n",
        "        \"No Movement\": mean_shap_abs_values[:, 1],\n",
        "        \"Decrease\": mean_shap_abs_values[:, 2],\n",
        "        \"Average\": shap_avg\n",
        "    })\n",
        "    df_shap.sort_values(by=\"Average\", ascending=False, inplace=True)\n",
        "\n",
        "\n",
        "    # Save the DataFrame to CSV if a file path is provided\n",
        "    if file_path:\n",
        "\n",
        "        df_shap.to_csv(os.path.join(file_path, f'Shap_Random_Forest_stock_{z}.csv'), index=False)\n",
        "        print(f\"Saved SHAP values to {file_path}\")\n",
        "    else:\n",
        "        print(\"No file path provided; CSV not saved.\")\n",
        "\n",
        "\n",
        "    return shap_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdBzSr9W_PNC"
      },
      "outputs": [],
      "source": [
        "def get_shap_resnet(model, X_train_scaled, X_test_scaled, feature_names, z,\n",
        "                    file_path='results/T1/stock_0/figures', background_size=50, EXPLAIN=20, ROUNDING=4):\n",
        "    # Convert 3D data (samples, features, channel) to 2D (samples, features)\n",
        "    X_train_2d = np.squeeze(X_train_scaled, axis=-1)\n",
        "    X_test_2d  = np.squeeze(X_test_scaled, axis=-1)\n",
        "\n",
        "    # Define a prediction wrapper that expands 2D input back to 3D for your model.\n",
        "    def model_predict_wrapper(X):\n",
        "        X_expanded = np.expand_dims(X, axis=-1)\n",
        "        return model.predict(X_expanded)\n",
        "\n",
        "    # Prepare background data: select a random subset from the training data\n",
        "    if X_train_2d.shape[0] > background_size:\n",
        "        indices = np.random.choice(X_train_2d.shape[0], background_size, replace=False)\n",
        "        background_data = X_train_2d[indices]\n",
        "    else:\n",
        "        background_data = X_train_2d\n",
        "\n",
        "    # Create an explicit masker using the background data.\n",
        "    masker = shap.maskers.Independent(background_data)\n",
        "\n",
        "    # Initialize the SHAP explainer with the prediction wrapper and masker.\n",
        "    explainer = shap.Explainer(model_predict_wrapper, masker, feature_names=feature_names)\n",
        "\n",
        "    # Compute SHAP values for the first EXPLAIN samples from the test set.\n",
        "    shap_values = explainer.shap_values(X_test_2d[:EXPLAIN])\n",
        "\n",
        "    # Compute mean absolute SHAP values per feature\n",
        "    # Handle multi-class case if shap_values is a list\n",
        "    if isinstance(shap_values, list):\n",
        "        if len(shap_values) > 1:\n",
        "            # Multi-class: stack each class's mean absolute SHAP values (resulting shape: (n_features, n_classes))\n",
        "            mean_shap_abs_values = np.stack(\n",
        "                [np.round(np.mean(np.abs(sv), axis=0), ROUNDING) for sv in shap_values], axis=1\n",
        "            )\n",
        "        else:\n",
        "            # Single class but provided in a list; expand dims to mimic 2D array\n",
        "            mean_shap_abs_values = np.round(np.mean(np.abs(shap_values[0]), axis=0), ROUNDING)\n",
        "            mean_shap_abs_values = mean_shap_abs_values[:, np.newaxis]\n",
        "    else:\n",
        "        # In case shap_values is not a list\n",
        "        mean_shap_abs_values = np.round(np.mean(np.abs(shap_values), axis=0), ROUNDING)\n",
        "        if mean_shap_abs_values.ndim == 1:\n",
        "            mean_shap_abs_values = mean_shap_abs_values[:, np.newaxis]\n",
        "\n",
        "    # Compute overall importance by averaging across classes\n",
        "    overall_importance = np.round(np.mean(mean_shap_abs_values, axis=1), ROUNDING)\n",
        "    overall_importance = np.asarray(overall_importance).flatten()\n",
        "\n",
        "    # Ensure feature_names is a list with the same length as the number of features\n",
        "    n_features = overall_importance.shape[0]\n",
        "    if len(feature_names) != n_features:\n",
        "        feature_names = list(feature_names)[:n_features]\n",
        "\n",
        "    # Compute an average SHAP value per feature across classes\n",
        "    shap_avg = np.mean(mean_shap_abs_values, axis=1)\n",
        "\n",
        "    # Create DataFrame.\n",
        "    # If we have exactly 3 classes, use the following column names.\n",
        "    if mean_shap_abs_values.shape[1] == 3:\n",
        "        df_shap = pd.DataFrame({\n",
        "            \"Feature\": feature_names,\n",
        "            \"Increase\": mean_shap_abs_values[:, 0],\n",
        "            \"No Movement\": mean_shap_abs_values[:, 1],\n",
        "            \"Decrease\": mean_shap_abs_values[:, 2],\n",
        "            \"Average\": shap_avg\n",
        "        })\n",
        "    else:\n",
        "        # For other numbers of classes, name the columns generically.\n",
        "        class_columns = [f\"Class_{i}\" for i in range(mean_shap_abs_values.shape[1])]\n",
        "        data = {\"Feature\": feature_names, \"Average\": shap_avg}\n",
        "        for i, col in enumerate(class_columns):\n",
        "            data[col] = mean_shap_abs_values[:, i]\n",
        "        df_shap = pd.DataFrame(data)\n",
        "\n",
        "    df_shap.sort_values(by=\"Average\", ascending=False, inplace=True)\n",
        "\n",
        "    # Save the DataFrame to CSV if a file path is provided\n",
        "    if file_path:\n",
        "        os.makedirs(file_path, exist_ok=True)\n",
        "        save_path = os.path.join(file_path, f'ResNet Stock {z}.csv')\n",
        "        df_shap.to_csv(save_path, index=False)\n",
        "        print(f\"Saved SHAP values to {save_path}\")\n",
        "    else:\n",
        "        print(\"No file path provided; CSV not saved.\")\n",
        "\n",
        "    return shap_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxMb1Y6R_PNC"
      },
      "outputs": [],
      "source": [
        "def get_shap_lstm(model, X_train_scaled, X_test_scaled, feature_names, z = 1, file_path='', background_size=50, EXPLAIN=20, ROUNDING=4):\n",
        "    # For run_lstm, data is expected to have shape (n_samples, 1, n_features).\n",
        "    # Squeeze the timestep dimension (axis=1) to obtain 2D arrays: (n_samples, n_features)\n",
        "    X_train_2d = np.squeeze(X_train_scaled, axis=1)\n",
        "    X_test_2d  = np.squeeze(X_test_scaled, axis=1)\n",
        "\n",
        "    # Define a prediction wrapper that adds the timestep dimension back (to shape: (batch, 1, n_features))\n",
        "    def model_predict_wrapper(X):\n",
        "        X_expanded = np.expand_dims(X, axis=1)\n",
        "        return model.predict(X_expanded)\n",
        "\n",
        "    # Prepare background data: randomly select from training data (2D)\n",
        "    if X_train_2d.shape[0] > background_size:\n",
        "        indices = np.random.choice(X_train_2d.shape[0], background_size, replace=False)\n",
        "        background_data = X_train_2d[indices]\n",
        "    else:\n",
        "        background_data = X_train_2d\n",
        "\n",
        "    # Create an explicit masker using the background data\n",
        "    masker = shap.maskers.Independent(background_data)\n",
        "\n",
        "    # Initialize the SHAP explainer with the prediction wrapper and masker\n",
        "    explainer = shap.Explainer(model_predict_wrapper, masker, feature_names=feature_names)\n",
        "\n",
        "    # Compute SHAP values for the first EXPLAIN samples from the test set (2D)\n",
        "    shap_values = explainer.shap_values(X_test_2d[:EXPLAIN])\n",
        "\n",
        "    # Compute overall mean absolute SHAP importance\n",
        "    if isinstance(shap_values, list):\n",
        "        if len(shap_values) > 1:\n",
        "            # Multi-class: for each class compute mean absolute values (shape: (n_features,))\n",
        "            mean_shap_class = [np.mean(np.abs(sv), axis=0) for sv in shap_values]\n",
        "            mean_shap_class = [np.round(arr, ROUNDING) for arr in mean_shap_class]\n",
        "            # Compute overall average importance by averaging across classes\n",
        "            overall_importance = np.round(np.mean(mean_shap_class, axis=0), ROUNDING)\n",
        "        else:\n",
        "            overall_importance = np.round(np.mean(np.abs(shap_values[0]), axis=0), ROUNDING)\n",
        "    else:\n",
        "        overall_importance = np.round(np.mean(np.abs(shap_values), axis=0), ROUNDING)\n",
        "\n",
        "    # Ensure overall_importance is 1D\n",
        "    overall_importance = np.asarray(overall_importance).flatten()\n",
        "\n",
        "    # Ensure feature_names is a list with the same length as the number of features\n",
        "    n_features = overall_importance.shape[0]\n",
        "    if len(feature_names) != n_features:\n",
        "        feature_names = list(feature_names)[:n_features]\n",
        "\n",
        "    mean_shap_abs_values = np.round(np.abs(shap_values).mean(axis=0), ROUNDING)\n",
        "    shap_avg = np.mean(mean_shap_abs_values, axis=1)\n",
        "\n",
        "\n",
        "    # Multi-class: assume 3 classes and create a DataFrame with per-class and overall averages.\n",
        "    # mean_shap_class = [np.round(np.mean(np.abs(sv), axis=0), ROUNDING).flatten() for sv in shap_values]\n",
        "    # print(mean_shap_class)\n",
        "\n",
        "    df_shap = pd.DataFrame({\n",
        "        \"Feature\": feature_names,\n",
        "        \"Increase\": mean_shap_abs_values[:, 0],\n",
        "        \"No Movement\": mean_shap_abs_values[:, 1],\n",
        "        \"Decrease\": mean_shap_abs_values[:, 2],\n",
        "        \"Average\": shap_avg\n",
        "    })\n",
        "    df_shap.sort_values(by=\"Average\", ascending=False, inplace=True)\n",
        "\n",
        "\n",
        "    # Save the DataFrame to CSV if a file path is provided\n",
        "    if file_path:\n",
        "\n",
        "        df_shap.to_csv(os.path.join(file_path, f'Shap_LSTM_stock_{z}.csv'), index=False)\n",
        "        print(f\"Saved SHAP values to {file_path}\")\n",
        "    else:\n",
        "        print(\"No file path provided; CSV not saved.\")\n",
        "\n",
        "\n",
        "    return shap_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SZeOtQb_PNC"
      },
      "source": [
        "# **Loop to Run Entire Pipeline**\n",
        "\n",
        "## ⚠️ Warning: This action will overwrite existing files.\n",
        "\n",
        "Main execution pipeline for end-to-end model training, evaluation, and result aggregation across multiple stock datasets.\n",
        "\n",
        "### Functionality:\n",
        "- Iterates over each folder in the `data/` directory (each representing a dataset).\n",
        "- Creates a full preprocessing folder structure, including:\n",
        "  - `Correlation_Matrix_Clean`\n",
        "  - `Correlation_Matrix_No_Corr`\n",
        "  - `Mid_Price_Graph`\n",
        "  - `VIF`\n",
        "- Loads and processes `train.csv` and `test.csv` files from each folder.\n",
        "- Cleans and prepares data using:\n",
        "  - Feature cleaning\n",
        "  - Correlation filtering\n",
        "  - Rolling window averaging\n",
        "- Optionally reduces data size or removes corrupt first rows.\n",
        "- Trains and evaluates multiple models per stock and per target horizon:\n",
        "  - Random Forest\n",
        "  - K-Means Clustering\n",
        "  - ARIMAX\n",
        "  - LSTM\n",
        "  - ResNet\n",
        "- Runs all models for three time horizons: `t+1`, `t+5`, and `t+10`.\n",
        "\n",
        "### Output:\n",
        "- Creates subfolders for each stock (`stock_0`, `stock_1`, ...) containing:\n",
        "  - A `results.csv` file with evaluation metrics per model\n",
        "  - A `figures/` folder with prediction plots\n",
        "- Aggregates all model results into a final `all_results.csv` sorted by accuracy.\n",
        "\n",
        "### Parameters:\n",
        "- `path_data` (default: `'data'`): Path to the top-level directory containing folders with `train.csv` and `test.csv` for each stock dataset.\n",
        "\n",
        "### Notes:\n",
        "- Automatically deletes and recreates the `Preprocessing/` directory if it already exists.\n",
        "- Logs errors per target if any model fails during training or prediction.\n",
        "- Model-specific SHAP visualisation code is included but currently commented out.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LprtM_k_PNC"
      },
      "outputs": [],
      "source": [
        "def run_pipeline(path_data = 'data'):\n",
        "\n",
        "    for folder_name in os.listdir(path_data):\n",
        "        folder_path = os.path.join(path_data, folder_name)\n",
        "        if os.path.isdir(folder_path):\n",
        "\n",
        "    ###################################################################################################################################\n",
        "    #\n",
        "    #                                                 CREATE FOLDERS FOR PREPROCESSING DATA\n",
        "    #\n",
        "    ###################################################################################################################################\n",
        "\n",
        "            path = os.path.join(folder_name, 'results')\n",
        "\n",
        "            # Define the full path for the \"Preprocessing\" folder\n",
        "            preprocessing_path = os.path.join(path, 'Preprocessing')\n",
        "\n",
        "            print(f\"Processing folder: {preprocessing_path}\")\n",
        "            # If the \"Preprocessing\" folder exists, remove it along with its contents\n",
        "\n",
        "            if os.path.exists(preprocessing_path):\n",
        "                shutil.rmtree(preprocessing_path)\n",
        "                print(f\"Deleted existing folder and its contents: {preprocessing_path}\")\n",
        "\n",
        "            # Create the \"Preprocessing\" folder\n",
        "            os.makedirs(preprocessing_path, exist_ok=True)\n",
        "            print(f\"Created folder: {preprocessing_path}\")\n",
        "\n",
        "            # List of subfolder names to create inside \"Preprocessing\"\n",
        "            subfolders = [\n",
        "                'Correlation_Matrix_Clean',\n",
        "                'Correlation_Matrix_No_Corr',\n",
        "                'Mid_Price_Graph',\n",
        "                'VIF'\n",
        "            ]\n",
        "\n",
        "            # Create each subfolder\n",
        "            for subfolder in subfolders:\n",
        "                subfolder_path = os.path.join(preprocessing_path, subfolder)\n",
        "                os.makedirs(subfolder_path, exist_ok=True)\n",
        "                print(f\"Created folder: {subfolder_path}\")\n",
        "\n",
        "    ###################################################################################################################################\n",
        "    #\n",
        "    #                                                 GATHER DATA TO RUN IN ML MODELS\n",
        "    #\n",
        "    ###################################################################################################################################\n",
        "\n",
        "\n",
        "            print(f\"Processing folder: {folder_path}\")\n",
        "            train_data = pd.read_csv(os.path.join(folder_path, 'train.csv'), delimiter=';')\n",
        "            test_data = pd.read_csv(os.path.join(folder_path, 'test.csv'), delimiter=';')\n",
        "            train_group_clean, test_group_clean, train_group_no_corr, test_group_no_corr, train_clean, test_clean, train_no_corr, test_no_corr = prepare_data_for_loop(train_data.T, test_data.T, preprocessing_path)\n",
        "\n",
        "            # TODO CHANGE Drop first line of the dataframes because np float 64 appears in the first line\n",
        "            train_group_clean = [df.drop(df.index[0]) for df in train_group_clean]\n",
        "            test_group_clean = [df.drop(df.index[0]) for df in test_group_clean]\n",
        "            train_group_no_corr = [df.drop(df.index[0]) for df in train_group_no_corr]\n",
        "            test_group_no_corr = [df.drop(df.index[0]) for df in test_group_no_corr]\n",
        "\n",
        "\n",
        "            # Used for debugging by running reduced set of data\n",
        "            # All models excluding ARIMAX\n",
        "            # train_group_clean = get_reduced_df(train_group_clean)\n",
        "            # test_group_clean = get_reduced_df(test_group_clean)\n",
        "            # train_group_no_corr = get_reduced_df(train_group_no_corr)\n",
        "            # test_group_no_corr = get_reduced_df(test_group_no_corr)\n",
        "\n",
        "            # Delete all folders in T1 first\n",
        "            # if os.path.exists(path):\n",
        "            #     for folder in os.listdir(path):\n",
        "            #         folder_path = os.path.join(path, folder)\n",
        "            #         print(folder_path)\n",
        "            #         if os.path.isdir(folder_path):\n",
        "            #             shutil.rmtree(folder_path)\n",
        "\n",
        "            # Loop stock\n",
        "            for i in range(len(train_clean)):\n",
        "                print(f\"\\nProcessing stock: {i}\\n\")\n",
        "                # Create a folder for the current stock\n",
        "                path_results = os.path.join(path, f'stock_{i}')\n",
        "                os.makedirs(path_results, exist_ok=True)\n",
        "\n",
        "\n",
        "                # Create folders for results and figures\n",
        "                path_figures = os.path.join(path_results, 'figures')\n",
        "                os.makedirs(path_figures, exist_ok=True)\n",
        "\n",
        "                # Create a CSV file path directly under stock folder\n",
        "                csv_file_path = os.path.join(path_results, 'results.csv')\n",
        "                with open(csv_file_path, 'w') as file:\n",
        "                    pass  # Empty CSV file creation\n",
        "\n",
        "                train = train_group_clean[i]\n",
        "                test = test_group_clean[i]\n",
        "\n",
        "                lst = ['Price Movement t+1','Price Movement t+5','Price Movement t+10']\n",
        "                # Run instant +1, +5, +10\n",
        "                # Loop through each column in the list\n",
        "                for tn in lst:\n",
        "                    use_train = train.copy()  # Copy original DataFrame\n",
        "                    use_test = test.copy()  # Copy original DataFrame\n",
        "                    use_train.drop(columns=[col for col in lst if col != tn], inplace=True)\n",
        "                    use_test.drop(columns=[col for col in lst if col != tn], inplace=True)\n",
        "\n",
        "                    # use_train = use_train[[tn]]  # Keep only the column in tn\n",
        "                    trainn = use_train  # Store the filtered DataFrame in trainn\n",
        "                    testt = use_test  # Store the filtered DataFrame in testt\n",
        "\n",
        "                    try:\n",
        "                        # run_linear_regression(trainn, testt, csv_file_path, path_figures)\n",
        "                        Random_Forest_data = run_random_forest(trainn, testt, csv_file_path, path_figures, tn)\n",
        "                        Kclustering_data = run_kclustering(trainn, testt, csv_file_path, path_figures, tn)\n",
        "                        ARIMA_data = run_arimax(trainn, testt, csv_file_path, path_figures, tn)\n",
        "                        LSTM_data = run_lstm(trainn, testt, csv_file_path, path_figures, tn)\n",
        "                        ResNet_data = run_resnet(trainn, testt, csv_file_path, path_figures, tn)\n",
        "\n",
        "                        ## Get SHAP values\n",
        "                        # csv_file_path = os.path.join(path_results, 'results.csv')\n",
        "                        # with open(csv_file_path, 'w') as file:\n",
        "                        #     pass  # Empty CSV file creation\n",
        "                        # get_shap_resnet(ResNet_data[0], ResNet_data[1], ResNet_data[2], ResNet_data[3])\n",
        "                        # get_shap(LSTM_data[0], LSTM_data[1], LSTM_data[2], LSTM_data[3])\n",
        "                    except Exception as e:\n",
        "                        print('##############################################')\n",
        "                        print('##############################################')\n",
        "                        print(f\"Error processing {tn}: {e}\")\n",
        "                        print('##############################################')\n",
        "                        print('##############################################')\n",
        "                        continue\n",
        "\n",
        "            all_rows = []\n",
        "\n",
        "            # Iterate through stock folders\n",
        "            for stock_folder in os.listdir(path):\n",
        "                stock_path = os.path.join(path, stock_folder)\n",
        "                if os.path.isdir(stock_path):\n",
        "                    csv_file = os.path.join(stock_path, 'results.csv')\n",
        "                    if os.path.exists(csv_file):\n",
        "                        df = pd.read_csv(csv_file,header=None)\n",
        "                        for _, row in df.iterrows():\n",
        "                            result_row = {\n",
        "                                'stock_number': stock_folder,\n",
        "                                'model': row[0],\n",
        "                                'accuracy': float(row[1]),\n",
        "                                'f1_score': float(row[2]),\n",
        "                                'kappa': float(row[3]),\n",
        "                                'precision': float(row[4])\n",
        "                            }\n",
        "                            all_rows.append(result_row)\n",
        "\n",
        "            ## Combine all rows into a DataFrame\n",
        "            if all_rows:\n",
        "                final_df = pd.DataFrame(all_rows)\n",
        "                final_df = final_df.sort_values(by='accuracy', ascending=False)\n",
        "\n",
        "                final_csv_path = os.path.join(path, 'all_results.csv')\n",
        "                final_df.to_csv(final_csv_path, index=False)\n",
        "                print(f\"All results combined and sorted by accuracy into {final_csv_path}\")\n",
        "            else:\n",
        "                print(\"No results.csv files found.\")\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE_fo-Cn_PND"
      },
      "source": [
        "# Generate Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dshdUuSbPNl2"
      },
      "outputs": [],
      "source": [
        "# results_df = pd.read_csv('model_results/model_kfold_results.csv')\n",
        "# results_df.head()\n",
        "\n",
        "# summary = results_df.groupby('model')['rmse'].agg(['mean', 'std']).reset_index()\n",
        "# print(\"\\nSummary statistics (average RMSE and standard deviation):\")\n",
        "# print(summary)\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.bar(summary['model'], summary['mean'], yerr=summary['std'], capsize=5, color='skyblue')\n",
        "# plt.xlabel('Model')\n",
        "# plt.ylabel('Average RMSE')\n",
        "# plt.title('Average RMSE per Model (with Error Bars)')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# results_df.boxplot(column='rmse', by='model', grid=False)\n",
        "# plt.xlabel('Model')\n",
        "# plt.ylabel('RMSE')\n",
        "# plt.title('RMSE Distribution per Model')\n",
        "# plt.suptitle('')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# best_model = summary.loc[summary['mean'].idxmin()]\n",
        "# print(f\"\\nBest model: {best_model['model']} with average RMSE of {best_model['mean']:.4f} (std={best_model['std']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRGIQO3H_PND"
      },
      "source": [
        "## Sectional run of the progam\n",
        "Run Section of the code with VS Code or Google Colab below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThmJnZ9B_PND"
      },
      "outputs": [],
      "source": [
        "# folder_path = '/content/drive/MyDrive/Dissertation/Zscore'\n",
        "\n",
        "# train_data = pd.read_csv(os.path.join(folder_path, 'train.csv'), delimiter=';', header=None)\n",
        "# test_data = pd.read_csv(os.path.join(folder_path, 'test.csv'), delimiter=';', header=None)\n",
        "\n",
        "# train_group_clean, test_group_clean, train_group_no_corr, test_group_no_corr, train_clean, test_clean, train_no_corr, test_no_corr = prepare_data_for_loop(train_data.T, test_data.T)\n",
        "\n",
        "\n",
        "\n",
        "# for folder_name in os.listdir(path):\n",
        "#     folder_path = os.path.join(path, folder_name)\n",
        "#     if os.path.isdir(folder_path):\n",
        "#         print(f\"Processing folder: {folder_path}\")\n",
        "#         train_data = pd.read_csv(os.path.join(folder_path, 'train.csv'))\n",
        "#         test_data = pd.read_csv(os.path.join(folder_path, 'test.csv'))\n",
        "#         train_clean, test_clean, train_no_corr, test_no_corr = prepare_data_for_loop(train_data.T, test_data.T)\n",
        "#         break\n",
        "#         # Add your processing code here\n",
        "# #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBxeFiAr_PND"
      },
      "source": [
        "## Get SHAP manually\n",
        "Below is the iteration used to fish out the SHAP values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl0hDJrT_PND"
      },
      "outputs": [],
      "source": [
        "# folder_path = 'Data\\T1'\n",
        "# train_data = pd.read_csv(os.path.join(folder_path, 'train.csv'), delimiter=';')\n",
        "# test_data = pd.read_csv(os.path.join(folder_path, 'test.csv'), delimiter=';')\n",
        "# train_group_clean, test_group_clean, train_group_no_corr, test_group_no_corr, train_clean, test_clean, train_no_corr, test_no_corr = prepare_data_for_loop(train_data.T, test_data.T, ' ')\n",
        "\n",
        "# train_group_clean = get_reduced_df(train_group_clean)\n",
        "# test_group_clean = get_reduced_df(test_group_clean)\n",
        "# train_group_no_corr = get_reduced_df(train_group_no_corr)\n",
        "# test_group_no_corr = get_reduced_df(test_group_no_corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCfmWoxR_PND"
      },
      "outputs": [],
      "source": [
        "\n",
        "# path_temp = 'temp'\n",
        "# csv_file_path_temp = os.path.join(path_temp, 'results.csv')\n",
        "# with open(csv_file_path_temp, 'w') as file:\n",
        "#     pass  # Empty CSV file creation\n",
        "\n",
        "# tr = train_group_clean[0]\n",
        "# te = test_group_clean[0]\n",
        "# lst = ['Price Movement t+1','Price Movement t+5']\n",
        "# # Run instant +1, +5, +10\n",
        "# # Loop through each column in the list\n",
        "\n",
        "# trr = tr.copy()  # Copy original DataFrame\n",
        "# tee = te.copy()  # Copy original DataFrame\n",
        "# trr.drop(columns=[col for col in lst], inplace=True)\n",
        "# tee.drop(columns=[col for col in lst], inplace=True)\n",
        "\n",
        "# a = run_arimax(trr, tee, csv_file_path_temp , 'temp', 'Price Movement t+10')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rIhQiH6_PND"
      },
      "source": [
        "## FUNCTION NOT USED IN CODE - CAN BE ADDED - Runs Kclustering\n",
        "\n",
        "Applies K-Means clustering to classify mid-price movement based on flattened, windowed feature data. Evaluates clustering performance against actual class labels.\n",
        "\n",
        "### Functionality:\n",
        "- Prepares time-series windowed data by flattening and scaling features.\n",
        "- Fits a `KMeans` clustering model on the training set using a specified number of clusters.\n",
        "- Assigns cluster labels to the test set and compares them to actual class labels.\n",
        "- Calculates evaluation metrics: Accuracy, F1 score (weighted), Cohen’s Kappa, and Precision.\n",
        "- Writes performance metrics to a CSV file and saves a prediction vs. actual comparison plot.\n",
        "\n",
        "### Parameters:\n",
        "- `train`: Training DataFrame with windowed or averaged features and a movement label.\n",
        "- `test`: Testing DataFrame with the same format.\n",
        "- `csv_file_path`: Path to the CSV file where evaluation results are saved.\n",
        "- `path_figures`: Directory to save the plot comparing predicted clusters and actual classes.\n",
        "- `tnn`: Identifier for tracking the time horizon or dataset (used in CSV output).\n",
        "- `n_clusters`: Number of clusters for K-Means (default: 3).\n",
        "\n",
        "### Output:\n",
        "- Appends performance metrics to a CSV file.\n",
        "- Saves a PNG plot showing predicted cluster labels vs actual class labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-xV4nG5_PND"
      },
      "outputs": [],
      "source": [
        "def run_kclustering(train, test, csv_file_path, path_figures, tnn, n_clusters=3):\n",
        "    def prepare_data(df):\n",
        "        X = np.stack([np.stack(row) for row in df.iloc[:, :-1].values], axis=0)\n",
        "        y = df.iloc[:, -1].values.astype(int)\n",
        "        return X, y\n",
        "\n",
        "    X_train, y_train = prepare_data(train)\n",
        "    X_test, y_test = prepare_data(test)\n",
        "\n",
        "    # Reshape to (samples, timesteps * features) for KMeans clustering\n",
        "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_flat)\n",
        "    X_test_scaled = scaler.transform(X_test_flat)\n",
        "\n",
        "    # KMeans clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(X_train_scaled)\n",
        "\n",
        "    # Predictions\n",
        "    predictions_class = kmeans.predict(X_test_scaled)\n",
        "\n",
        "    # Metrics calculation\n",
        "    accuracy = accuracy_score(y_test, predictions_class)\n",
        "    f1 = f1_score(y_test, predictions_class, average='weighted')\n",
        "    kappa = cohen_kappa_score(y_test, predictions_class)\n",
        "    precision = precision_score(y_test, predictions_class, average='weighted')\n",
        "\n",
        "    # Write metrics to CSV\n",
        "    with open(csv_file_path, 'a') as file:\n",
        "        file.write(f'kclustering {tnn},{accuracy},{f1},{kappa},{precision}\\n')\n",
        "\n",
        "    # Plotting results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(y_test, label='Actual', linestyle='dashed')\n",
        "    plt.plot(predictions_class, label='Predicted (Clusters)', linestyle='solid')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Cluster/Class')\n",
        "    plt.title('KMeans Clustering Predictions vs Actual Classes')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(path_figures, 'kmeans_clustering.png'))\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_nbPN0C_PND"
      },
      "source": [
        "## FUNCTION NOT USED IN CODE - CAN BE ADDED - Runs linear regression\n",
        "\n",
        "Trains and evaluates a linear regression model for multi-class classification using windowed feature data. Outputs model performance metrics and a comparison plot.\n",
        "\n",
        "### Functionality:\n",
        "- Prepares windowed feature data by flattening it into 2D format suitable for linear regression.\n",
        "- Scales the input features using `StandardScaler`.\n",
        "- Trains a `LinearRegression` model on the processed training data.\n",
        "- Predicts class values for the test set and rounds predictions to nearest class (0, 1, or 2).\n",
        "- Calculates classification metrics: Accuracy, F1 score (weighted), Cohen’s Kappa, and Precision.\n",
        "- Appends results to a CSV file.\n",
        "- Saves a plot comparing predicted vs. actual class labels.\n",
        "\n",
        "### Parameters:\n",
        "- `train`: Preprocessed training DataFrame with windowed features and a single movement label column.\n",
        "- `test`: Preprocessed testing DataFrame in the same format.\n",
        "- `csv_file_path`: File path for storing model evaluation metrics.\n",
        "- `path_figures`: Directory path to save the prediction comparison plot.\n",
        "\n",
        "### Output:\n",
        "- Appends a row of performance metrics to the specified CSV file.\n",
        "- Saves a PNG plot comparing actual vs. predicted class labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb0nWDlz_PND"
      },
      "outputs": [],
      "source": [
        "def run_linear_regression(train, test, csv_file_path, path_figures):\n",
        "    def prepare_data(df):\n",
        "        X = np.stack([np.stack(row) for row in df.iloc[:, :-1].values], axis=0)\n",
        "        y = df.iloc[:, -1].values.astype(int)\n",
        "        return X, y\n",
        "\n",
        "    X_train, y_train = prepare_data(train)\n",
        "    X_test, y_test = prepare_data(test)\n",
        "\n",
        "    # Reshape to (samples, timesteps * features) for linear regression\n",
        "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_flat)\n",
        "    X_test_scaled = scaler.transform(X_test_flat)\n",
        "\n",
        "    # Linear regression model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    predictions = model.predict(X_test_scaled)\n",
        "    predictions_class = np.clip(np.round(predictions), 0, 2).astype(int)  # Ensure predictions are valid classes (0,1,2)\n",
        "\n",
        "    # Metrics calculation\n",
        "    accuracy = accuracy_score(y_test, predictions_class)\n",
        "    f1 = f1_score(y_test, predictions_class, average='weighted')\n",
        "    kappa = cohen_kappa_score(y_test, predictions_class)\n",
        "    precision = precision_score(y_test, predictions_class, average='weighted')\n",
        "\n",
        "    # Write metrics to CSV\n",
        "    with open(csv_file_path, 'a') as file:\n",
        "        file.write(f'linear_regression,{accuracy},{f1},{kappa},{precision}\\n')\n",
        "\n",
        "    # Plotting results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(y_test, label='Actual', linestyle='dashed')\n",
        "    plt.plot(predictions_class, label='Predicted', linestyle='solid')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Class')\n",
        "    plt.title('Linear Regression Predictions vs Actual Classes')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(path_figures, 'linear_regression_classification.png'))\n",
        "    plt.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}